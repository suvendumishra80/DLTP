################## Objective of this script #######
# Pre-requisite:
#  1) Default R version from R-Studio to be set to C:\Program Files\R\R-3.6.2
#  2) Restrart the R-Studio to take this in effect

# This Test script will:
#  1) Reconciliation of data
#

##################################:
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m")) 
options(warn = -1) # disable warning messages
invisible(gc))

#print(memory.size())
#################################################################Functions - START #############################################################
### Install or import libraries 
myPackageAndLibraries =  function(m_libPath,m_repoPath){
	# m_libPath = "C:/SWDTOOLS/Rtools/win-library/3.6" # in windows
	# m_libPath in linux: "/usr/XXXX/R/3.6"
	# in windows & linux :
	# m_repoPath = "https://efx-nexus.systems.uk.hsbc:8084/nexus/content/repositories/cloud-r-project-proxy/" 
	existing_libs = .libPaths(c(m_libPath,.libPaths()))
	# m_repoPath = "https://efx-nexus.systems.uk.hsbc:8084/nexus/content/repositories/cloud-r-project-proxy/" 

	#print(existing_libs)  
	#print("########################....1")
	#print(installed.packages(lib.loc-existing_libs)[,"Package"])
	
	#print("########################....2")
	#Sys.setenv(GCS_AUTH_FILE = "C:\\01-Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins\\auth_keys\\hsbc-9594666- refdatahub2-dev-b85cae9805a2.json")
	list.of.packages <- c(	"jsonlite",
							"httr",
							"rJava",
							"RJDBC",
							"ggplot2",
							"assertr",
							"RODBC",
							"dplyr",
							"tidyselect",
							"tidyr",
							"string",
							"stringi",
							"openxlsx",
							"lubridate",
							"openssl",
							"gridExtra",
							"grid",
							"readr",
							"xml2",
							"stats",
							"getPass"
							#"DBI",
							#"digest",
							#"caTools",
							#"readtext",
							#"rstudioapi",
							#"shiny",
							#"readxl",
							#"googleCloudStorageR",
							#"gargle",
							#"fs",
							#"Rcpp",
							#"remotes",
							#"usethis",
							#"reshape2",
							#"tools",
							#"purrr",
							#"lifecycle" 
						)

	#pack as.data.frame(available.packages())
	#print(pack[pack$Package %in% list.of.packages,c("Depends", "Imports")])
	new.packages <- list.of.packages[! (list.of.packages %in% installed.packages(lib.loc-existing_libs) [,"Package"])] 
	#print("########################....3")
	if(length(new.packages)){
		#print("############....4")

		trycatch({install.packages(new.packages,
								repos =m_repoPath,
								lib=m_repopath,
								dependencies = True)}, error=function(cond){install.packages(new.packages,
																											repos=m_repoPath,
																											dependencies = TRUE)})
	}
	#print("######################...5")

	for(package_name in list.of.packages){
		supressMessages(supressWornings(library(Package_name,character.only-True,lib.loc =existing_libs)))
		#library(package_name,character.only-TRUE,quietly=FALSE,lib.loc =existing_libs)
		}
		#print("#################...6")
}
## *************

optimal.scale <- function(w,h, wanted.w, wanted.h) max(c(w/wanted.w, h/wanted.h))

## *************
getDBServerDate = function(this_DSN,this_query){
	connectionstring =NA
	conn =NA
	data =NA
	#library(RODBC)
	#this_DSN =whichDSN
	#this_Query = "SELECT CURRENT_TIMESTAMP TEST_EXEC_TIME FROM sysibm.sysdummy1"
	
	connectionString = paste("DSN", this_DSN, ";",sep="")
	conn =RODBC::odbcDriverConnect(connectionString)
	data =RODBC::sqlQuery(conn,query=this_Query,error = TRUE)
	RODBC::odbcCloseAll()
	data = data[,"TEST_EXEC_TIME"]
	
	return(data)
	
	}
#####################
separate_Transformation_Strings = function(StriporConcat_NativeFunction=c("Strip","concat"),m_format,m_string,m_new_string){
	m_str = str_sp;it(unlist(str_split(m_String,"\\)")),"\\(")
	
	m_len=length(m_str[[1]])
	m_str_str=m_str[[1]][which(m_str[[1]]==m_format):m_len]
	m_str_str_collapse = paste0(m_str_str,collapse = "(")}
	m_final_xml=ifelse(length(m_str_str)==3,paste0(m_str_str_collapse,""))"),paste0(m_str_str_collapse,")"))

	
	if(striporConcat_NativeFunction=="Strip"){
		m_String = m_final_xml
	}else{
		#print(m_new_string)
		a = paste0(m_str[[1]][1:which(m_str[[1]]==m_format)-1],collapse = "(")
		a = ifelse(a=="",a,paste0(a,"("))
		b = ifelse(length(m_str_str)==3,stri_replace_last_fixed(m_new_string,")",)
	)}
dependencies = TRUE)}, error=function(cond) {install.packages(new.packages,
}
#print("######### ###########....5")
repos = m_repoPath,
dependencies = TRUE)})
for(package_name in list.of.packages){
suppressMessages(suppressWarnings(library(package_name,character.only=TRUE, lib.loc -existing_libs )))
#library(package_name,character.only=TRUE, quietly=FALSE,lib.loc =existing_libs)
}
#print("##########
}
## **
############....6")
**
optimal.scale <- function(w,h, wanted.w, wanted.h) max(c(w/wanted.w, h/wanted.h))
##
***
getDBServerDate = function(this_DSN,this_Query){
}
connectionString = NA
conn = NA
data = NA
#library(RODBC)
#this_DSN =which DSN
#this_Query "SELECT CURRENT_TIMESTAMP TEST_EXEC_TIME FROM sysibm.sysdummy1" connectionString = paste("DSN=",this_DSN,";",sep="")
conn = RODBC::odbcDriverConnect(connectionString)
data = RODBC::sqlQuery(conn,query=this_Query,errors = TRUE)
RODBC::odbcCloseAll()
data = data[,"TEST_EXEC_TIME"]
return(data)
############
-
separate_Transformation_Strings =
function(StripOrConcat_Native Function=c("Strip","Concat"),m_format,m_String,m_new_string){
m_str = str_split(unlist(str_split(m_String,"\\")),"\\(")
m_len-length(m_str[[1]])
m_str_str=m_str[[1]][which(m_str[[1]]==m_format):m_len]
m_str_str_collapse = paste0(m_str_str,collapse = "(")
m_final_xml=ifelse(length(m_str_str)==3,paste0(m_str_str_collapse,"))"), paste0(m_str_str_collapse,")"))
if(StripOrConcat_Native Function=="Strip"){
m_String = m_final_xml
}else{
}
#print(m_new_string)
a = paste0(m_str[[1]][1:which(m_str[[1]]==m_format)-1], collapse = "(")
a = ifelse(a=="",a,paste0(a,"("))
b = ifelse(length(m_str_str)==3,stri_replace_last_fixed (m_new_string,")",""),m_new_string)
c = paste0(m_str[2:length(m_str)], collapse=")")
m_String = paste0(a,b,c)
return(m_String)
} #end function
############
parseTransformationColumn = function(m_paramters_list){
### Check validating of content for XML, JSON, FWF
##### For XML/JSON,
####### if unnest or xpath key words are mentioned then do not parse
####### if keyword XML(fieldname,'//parent1//parent2//parent3/child', 'varchar(255))') is not found exit with default
####### make sure it has XML as key word with 3 parameters
####### if data type is not given than convert it to default text (e.g in postgre)
### make sure parent nodes has 2 -> // and last level value node has 1->/,
####### if intermediate nodes has 1->/ make sure its corrected internally #print("...12.9.1")
#inside function
# m_String=
m_attribute_rowID = m_paramters_list$m_attr_id
m_DBProvider = m_paramters_list$m_db_provider
m_fieldName = m_paramters_list$m_attr_name
m_tableName = paste0(m_paramters_list$m_table,".csv") m_String = m_paramters_list$m_attr_transformation print(m_String)
m_input_folder = m_paramters_list$m_input_file_folder_path m_output_folder = m_paramters_list$m_output_file_folder_path m_google_bin_path= m_paramters_list$m_google_sdk_bin_path
#
m_fileType=""
m_sourceType="" m_isHeader=""
m_sourcePathorFieldName=""
m_fileSearchPattern=""
m_containsString=""
m_startIndex = 0
m_endIndex=0
df_FieldData=NA
m_xmlPath=""
m_dataType=""
m_otherFunctions=""
m_new_string=""
return TransColumn=""
m_format=""
if(m_DBProvider=="PostgreSQL"){
m_dataType="text"
#print("...12.9.2")
if(str_detect(m_String, "JSON\\(") & !(str_detect(m_String,"jsonb_each_text") |
str_detect(m_String,"json_array_elements") | str_detect(m_String,"json_extract_path") |
str_detect(m_String,"json_extract_path_text")|
str_detect(m_String,"jsonb_array_elements"))) { ## posgresql: JSON
#print("Lets proceed for JSON")
m_format = "JSON"
#print("...12.9.3...JSON")
}else if(str_detect (m_String, "XML\\(") & !str_detect(m_String,"xpath")){
#print("Lets proceed for XML")
m_format = "XML"
#print("...12.9.3...XML")
}else{
}
m_format=""
returnTransColumn = m_String
#print("Return String AS is")
if(m_format != ""){
m_Mod_String = separate_Transformation_Strings(StripOrConcat_Native Function ="Strip",m_format, m_String,m_new_string)
#print("...12.9.4")
m_temp = str_replace_all(m_Mod_String,""","")
m_temp = str_replace_all(m_temp, paste0(m_format,"\\"),"")
m_temp = str_replace_all(m_temp,"\\)","")
m_temp = unlist(str_split(m_temp,","))
#print("...12.9.5")
## IF JSON
if(m_format=="JSON"){
## XML/JSON Field Name
m_fieldName = m_temp[1]
m_fieldName = str_replace_all(m_fieldName, ":","")
#m_fieldName = paste0(m_fieldName, paste0(":", str_to_lower(m_format))) #print("...12.9.6")
## XML/JSON Path
m_xmlPath = m_temp[2]
m_temp_xml = unlist(str_split(m_xmlPath,"/"))
m_temp_xml = m_temp_xml [which(m_temp_xml!="")]
m_length = length(m_temp_xml)
m_SingleOrMultiJSONAttribute = str_to_lower(m_temp[3])
111111
m_SingleOrMultiJSONAttribute = ifelse(m_SingleOrMultiJSONAttribute== 11
#print("...12.9.7")
is.na(m_SingleOrMultiJSONAttribute) ||
m_SingleOrMultiJSONAttribute =="","single",m_SingleOrMultiJSONAttribute)
if(m_SingleOrMultiJSONAttribute=="multiple"){
#if(m_length >= 2){
m_fieldName = paste0(m_fieldName) #::jsonb
# NEW FORMAT BELOW
#jsonb_array_elements(inst_source_definition->'businessObject'->0->'instrumentIdentifier')::jsonb->>'idType' as
idType1,
#m_temp_xml = paste0("'",paste0(m_temp_xml[-m_length], collapse="->""),"')::jsonb->>"",m_temp_xml [m_length],'"'""') #m_temp_xml = paste0("jsonb_array_elements(",m_fieldName,"->",m_temp_xml)
m_temp_xml_txt=""
for(i in 1:(m_length-1)){
}
a = ifelse(is.na(as.integer(m_temp_xml[i])),
paste0("'",m_temp_xml [i],"""),
as.integer(m_temp_xml[i]))
m_temp_xml_txt = paste(m_temp_xml_txt,a, sep="->") #print(m_temp_xml_txt)
m_temp_xml = paste0("jsonb_array_elements(",m_fieldName,m_temp_xml_txt,")::jsonb- >>"",m_temp_xml [m_length],""")
}else{
}
m_fieldName = paste0(m_fieldName,paste0(":",str_to_lower(m_format),"b")) #::jsonb
#json_extract_path_text(source_definition::json, 'regionName')
if(m_length >= 2){
m_temp_xml = paste0(""",paste0(m_temp_xml [-m_length], collapse="","","","",m_temp_xml [m_length],"')") }else{
}
m_temp_xml = paste0(""",m_temp_xml [m_length],"")")
m_temp_xml = paste0("jsonb_extract_path_text(",m_fieldName,",",m_temp_xml)
#m_temp_xml = paste0("jsonb_extract_path_text(",m_fieldName,","",m_temp_xml[m_length],"')")
m_xmlPath = m_temp_xml
#print("...12.9.9")
if(length(m_temp)==2){
m_dataType = "text"
}else{
}
m_dataType = m_temp[4]
if(str_detect(m_dataType,"\\("))}{
}
m_dataType = paste0(m_dataType,")")
#print("...12.9.10...JSON end")
}else if(m_format=="XML"){## JSON above end, XML start
## XML/JSON Field Name
m_fieldName = m_temp[1]
m_fieldName = str_replace_all(m_fieldName,":","")
m_t
_field Name = paste0(m_fieldName,paste0(":", str_to_lower(m_format))) #print("...12.9.6")
## XML/JSON names space specifically for XML
m_xmlns= m_temp[2]
if(is.na(m_xmlns) || m_xmlns==""){
m_xmlns=NA
}else{
}
m_xmlns= paste0("{{n,",m_xmlns,"}}")
#print(paste0("Namesspace=",m_xmlns))
## XML/JSON Path
m_xmlPath = m_temp[3]
m_temp_xml = unlist(str_split(m_xmlPath,"/"))
m_temp_xml = m_temp_xml [which(m_temp_xml!="")]
m_length = length(m_temp_xml)
#m_temp_xml = "//Accounts"
#print("...12.9.7")
m_SingleOrMultiXMLAttribute = str_to_lower(m_temp[4])
m_SingleOrMultiXMLAttribute = ifelse(m_SingleOrMultiXMLAttribute=='
is.na(m_SingleOrMultiXMLAttribute) ||
111111
11
m_SingleOrMultiXMLAttribute =="","single", m_SingleOrMultiXMLAttribute)
if(is.na(m_xmlns)){
#if(m_length >= 2){
if(m_SingleOrMultiXMLAttribute=="multiple"){
m_temp_xml = paste0("//", paste0(m_temp_xml [-m_length], collapse="//"),"/",m_temp_xml [m_length]) m_temp_xml = paste0("unnest(xpath("",m_temp_xml,"/text()",",m_fieldName,"))")
}else{
}
m_temp_xml = paste0("(xpath(","//",m_temp_xml[m_length],"/text()",",m_fieldName,"))[1]")
}else{
}
#if(m_length >= 2){
if(m_SingleOrMultiXMLAttribute=="multiple"){
m_temp_xml = paste0("//n:",paste0(m_temp_xml [-m_length], collapse="/n:"),"/n:",m_temp_xml [m_length]) m_temp_xml = paste0("unnest(xpath("",m_temp_xml,"/text()",",m_fieldName,","",m_xmlns,"'))")
}else{
}
m_temp_xml = paste0("(xpath(","//n:",m_temp_xml [m_length],"/text()",",m_fieldName,","",m_xmlns,""'))[1]")
#print("...12.9.8...XML")
m_xmlPath = m_temp_xml
#print("...12.9.9")
if(length(m_temp)==4){
m_dataType = "text"
}else{
}
m_dataType = m_temp[5]
if(str_detect(m_dataType,"\\{(")){
}
m_dataType = paste0(m_dataType,")")
#print("...12.9.10")
}## XML end
#print(m_fieldName)
#print(m_xmlPath)
#print(m_dataType)
m_new_string = paste0(m_xmlPath," ::",m_dataType)
#print(m_new_string)
returnTransColumn = separate_Transformation_Strings (StripOrConcat_Native Function ="Concat",m_format, m_String,m_new_string)
#print("...12.9.11")
#print(returnTransColumn)
}## m_format=XML/JSON/None
}else if(m_DBProvider=="FILE"){
if(str_detect(m_String, "FILE.*FIXED_W")){ # Fixed Width
#print("Lets proceed for FIXED WIDTH")
m_format = "FIXED W"
}else if(str_detect(m_String,"FILE.*TAB")){
m_format = "TAB"
}else if(str_detect(m_String,"FILE.*COMMA")){
}
m_format = "COMMA"
print(m_format)
m_Mod_String = separate_Transformation_Strings (StripOrConcat_Native Function ="Strip","FILE", m_String,m_new_string) print(m_Mod_String)
m_temp = str_replace_all(m_Mod_String,"","")
m_temp = str_replace_all(m_temp, paste0(m_format,"\\"),"")
m_temp = str_replace_all(m_temp,"\\)","") m_temp = unlist(str_split(m_temp,","))
# sourceType
m_fileType = str_replace(m_temp[1],"\\{","_")
#print(m_fileType)
m_sourceType = m_temp[2]
m_sourcePathorField Name = m_temp[3]
m_fileSearchPattern = m_temp[4]
m_containsString = m_temp[5]
if(m_fileType=="FILE_FIXED_W"){
m_startIndex =as.integer(m_temp[6]) # should be integer m_endIndex = as.integer(m_temp[7]) # should be integer
}else if(m_fileType=="FILE_TAB"){
}
m_isHeader = as.logical (m_temp[6]) # should logical for isheader
else if(m_fileType=="FILE_COMMA"){ #
FILE('COMMA','FEED/GCP','/GCPBucketPath','*.csv','containSTring', 'isHeader(True/False') m_isHeader = as.logical (m_temp[6])# should logical for isheader
m_fileSearchPattern_t = str_remove_all(m_fileSearch Pattern,"\\*") m_fileSearchPattern_t = unlist(str_split(m_fileSearch Pattern_t,"\\.")) if(m_sourceType=="GCP"){ # GCP
#print("Lets proceed with GCP Connection")
if(m_attribute_rowID==1){ ## when things has to be done only one time
#### Make sure file is same or not at local vs gcp
if(file.exists(paste0(BUCKET_FOLDER,"\\manifest.csv"))){ # gcp file copy manifest available
df_manifest = read.csv(paste0(BUCKET_FOLDER,"\\manifest.csv"))
df_manifest = df_manifest %>% filter (Destination==m_fileSearchPattern) if(nrow(df_manifest)>0){ # if found matching files in local manifest
i=1
for(i in 1:nrow(df_manifest)){
m_destination_filepath = df_manifest[i,"Destination"]
m_destination_filepath = str_remove(m_destination_filepath,"file://") m_manifest_hash= df_manifest[i,"Md5"]
a=shell(paste("call ",m_google_bin_path,"gsutil hash -m",
m_destination_filepath,"
>",BUCKET_FOLDER,"\\manifest_next.csv"))
m_manifesh_dest_hash = read.csv(paste0(BUCKET_FOLDER,"\\manifest_next.csv")) names(m_manifesh_dest_hash) = c("hash")
m_manifesh_dest_hash$hash = str_remove_all(m_manifesh_dest_hash$hash,"\t")
m_manifesh_dest_hash$hash = str_replace(m_manifesh_dest_hash$hash, "Hash \\(md5\\):","")
if(m_manifest_hash==m_manifesh_dest_hash[1,"hash"]){
print("No change in local & GCP file..no need to download.")
}else{
m_filename = basename(m_destination_filepath)
print("Local & GCP file different..Download required !.") a=shell(paste0("call ",m_google_bin_path, "gsutil cp ",
m_sourcePathorFieldName,"/",m_filename,
BUCKET_INPUT_FOLDER))
} # if/else local & gcp file hash equal or not
}# end for
}else{ # download fresh from GCP with new manifest
11 11
#### WRITE A UNITILITY TO DOWNLOAD/COPY THE FILES FROM BUCKET TO BUCKET_INPUT_FOLDER folder
}
#m_filename = basename(m_destination_filepath) a=shell(paste("call ",m_google_bin_path,"gsutil cp -L",
BUCKET_FOLDER,"\\manifest.csv",
"",m_sourcePathorFieldName,"/",m_fileSearchPattern,
BUCKET_INPUT_FOLDER))
11 11
}else{
#### WRITE A UNITILITY TO DOWNLOAD/COPY THE FILES FROM BUCKET TO BUCKET_INPUT_FOLDER folder #m_filename = basename(m_destination_filepath)
a=shell(paste("call ",m_google_bin_path,"gsutil cp -L",
BUCKET_FOLDER,"\\manifest.csv",
11
",m_sourcePathorField Name,"/",m_fileSearchPattern, "", BUCKET_INPUT_FOLDER))
}
## Once downloaded, consolidate all files at one text file
list_of_files_path = list.files(paste0(m_input_folder), full.names = TRUE) list_of_files = list.files (paste0(m_input_folder), full.names = FALSE)
m_fileName_with_Path = data.frame("name"=list_of_files,"path"-list_of_files_path)
#print(m_fileName_with_Path)
if(length(m_fileSearch Pattern_t)>0){
m_pattern=""
for(m_pattern in m_fileSearchPattern_t){
#print(m_pattern)
m_fileName_with_Path = m_fileName_with_Path[which(m_fileName_with_Path$name %in% filter(function(x) grepl(m_pattern, x), m_fileName_with_Path$name) ),]
}
}
#print(m_fileName_with_Path)
## CONDILIDTATE TO ONE FILE: Assumtion: All file are fixed lenth file
#for(m_file in list_of_files_path){
# print(m_file)
#print("...1")
if(file.exists(paste0(BUCKET_FOLDER,"\\",m_tableName))){
}
file.remove(paste0(BUCKET_FOLDER,"\\",m_tableName))
#print(paste0("Removed Table file...=",m_tableName))
#print("...2")
if(file.exists(paste0(BUCKET_FOLDER,"\\", "Consolidated Files.txt"))){
}
file.remove(paste0(BUCKET_FOLDER,"\\", "Consolidated Files.txt"))
#print(paste0("Removed Consolidated file...=","Consolidated Files.txt"))
#print("...3")
m_data=""
for(m_file in m_file Name_with_Path$path){
}
print(m_file)
m_data = read Lines(m_file, warn = FALSE)
write_lines(m_data, paste0(BUCKET_FOLDER,"\\Consolidated Files.txt"), append = TRUE)
#print("...4")
#write_lines(m_data,paste0(BUCKET_FOLDER,"\\Consolidated Files.txt"))
}## if first time per table
#print("...5")
m_contents = read Lines (paste0(BUCKET_FOLDER,"\\Consolidated Files.txt"))
#print("...6")
if(is.na(m_containsString) || m_containsString==""){
#m_contents = m_contents
#print("...7.1")
}else{
}
#print(paste0("...7.2", length(m_contents),m_containsString))
m_contents = m_contents[stri_startswith(m_contents, fixed-m_containsString)==TRUE] #print(paste0("...7.3", length(m_contents),m_containsString))
#print("...8")
if(m_fileType=="FILE_FIXED_W"){
df_FieldData = read_fwf(m_contents,
fwf_positions(start = m_startIndex,end = m_endIndex,col_names = m_fieldName), skip = 0)
}else if(m_fileType=="FILE_TAB"){
#print(m_fieldName)
#print(m_isHeader)
#print(length(m_contents))
df_FieldData = read_delim (m_contents,col_names = m_isHeader,delim = "\t")
#df_FieldData = df_Field Data %>% mutate (across(everything(), as.character)) #print(head(df_FieldData))
df_FieldData = df_FieldData[,m_fieldName]
}else if(m_fileType=="FILE_COMMA"){
}
df_FieldData = read_delim (m_contents,col_names = m_isHeader,delim = ",") df_FieldData = df_Field Data[,m_fieldName]
#print("...9")
if(file.exists(paste0(BUCKET_FOLDER,"\\",m_tableName))){
df_temp = read.csv(paste0(BUCKET_FOLDER,"\\",m_tableName),
check.names=FALSE)
df_temp = cbind(df_temp,df_FieldData)
#print(head(df_temp))
write.csv(df_temp,paste0(BUCKET_FOLDER,"\\",m_tableName), row.names = FALSE) #print(paste0("Appending Column...=",names(df_Field Data),"in Table =",m_tableName))
}else{
}
write.csv(df_Field Data, paste0(BUCKET_FOLDER,"\\",m_tableName), row.names = FALSE) #print(paste0("First time created ...",m_tableName))
#print("...10")
m_new_string = m_fieldName
=
returnTransColumn separate_Transformation_Strings(StripOrConcat_Native Function ="Concat", "FILE", m_String,m_new_string)
} else{ # if FILE --> FEED
print("Lets proceed for FILE --> FEED")
returnTransColumn = m_String
} # FILE --> GCP/FEED end
#m_format for FILE provider
} ## if m_DBProvider = FILE
else{#not postgresql, but any other db
returnTransColumn = m_String
#print("Return String AS is")
} ## Not postgreSQL
#print("...12.9.12")
print(returnTransColumn)
returnTransColumn=toString(return TransColumn)
#print("...12.9.13")
return(returnTransColumn)
} #end function
############
##
**
executeCode= function(df_Conn_Validation_In_Scope,
df_Data_Lineage_5,
df_Data_Lineage_Constraints_Map_6, df_Data_Lineage_Reference_Data_Map_7){
#print("...2")
GP DATALOAD TYPE=NA
GP_LAST_DAY1_INCREMENTAL_DATE=NA
GP_LAST_DAYO_CUTOFF_DATE = NA
#So that data flow sequence for intermediate layer is maintained, 1 means base layer df_Conn_Validation_In_Scope = arrange (df_Conn_Validation_In_Scope, Layer_SEQ)
GP_DATALOAD_TYPE = unique (df_Conn_Validation_In_Scope$DATALOAD_TYPE)
#View(df_Conn_Validation_In_Scope)
if(GP_DATALOAD_TYPE=="INCREMENTAL"){ ## Apply CDC logic - Day 1- based on prev_exec_date
#names(df_Conn_Validation_In_Scope)
GP_LAST_DAY1_INCREMENTAL_DATE = unique (df_Conn_Validation_In_Scope [1,"GP_PREV_TEST_EXEC_TIME"]) GP_LAST_DAY1_INCREMENTAL_DATE = str_replace(GP_LAST_DAY1_INCREMENTAL_DATE,"","")
#print(GP_LAST_DAY1_INCREMENTAL_DATE)
if(is.na(GP_LAST_DAY1_INCREMENTAL_DATE) || is.null(GP_LAST_DAY1_INCREMENTAL_DATE) ||
GP_LAST_DAY1_INCREMENTAL_DATE==""){ ## First time execution, get the server date and reduce 1 day
whichLayer = toString(unique (df_Conn_Validation_In_Scope [1,"DERIVE_RESULTFILENAME_LAYER"])) whichDSN =
toString(df_Conn_Validation_In_Scope [df_Conn_Validation_In_Scope$Layer==which Layer, "ODBC_DSN_Name"]) whichQuery = toString(unique(df_Conn_Validation_In_Scope [1,"SQL_GET_SOURCE_SERVER_TIME"])) m_date_time = getDBServer Date (which DSN, whichQuery)
temp_date = as. POSIXct(m_date_time) - as.difftime(1, unit="days")
temp_date = format(temp_date,"%Y-%m-%d %H:%M:%OS3") #m_date_time = m_date_time - 1 # yesterdays date
#df_Conn_Validation_In_Scope$GP_PREV_TEST_EXEC_TIME = temp_date GP_LAST_DAY1_INCREMENTAL_DATE=temp_date
}else{
## Do not do anything but, after reconciliation, update this GP_PREV_TEST_EXEC_TIME GP_LAST_DAY1_INCREMENTAL_DATE =
str_replace(toString(unique (df_Conn_Validation_In_Scope [1,"GP_PREV_TEST_EXEC_TIME"])),"'","")
}
}else{ ## FULL LOAD - Day 0 - checks
## May CUTOFF LOGIC CAN BE WRITTEN HERE
GP_DATALOAD_TYPE = "FULL_LOAD"
if(is.na(GP_LAST_DAY1_INCREMENTAL_DATE) || is.null(GP_LAST_DAY1_INCREMENTAL_DATE) || GP_LAST_DAY1_INCREMENTAL_DATE==""){
GP_LAST_DAYO_CUTOFF_DATE=NA
}else{
GP_LAST_DAY1_INCREMENTAL_DATE =
str_replace(toString(unique(df_Conn_Validation_In_Scope [1,"GP_PREV_TEST_EXEC_TIME"])),"'","")
GP_LAST_DAYO_CUTOFF_DATE=GP_LAST_DAY1_INCREMENTAL_DATE
}
}
#print("...3.1")
original_column_names_of_constraint_sheet = names(df_Data_Lineage_Constraints_Map_6)
#print("...3.2")
## print(original_column_names_of_constraint_sheet)
df_Data_Lineage_Constraints_Map_6$Constraint_RecordID= seq.int(from =
1,to=nrow(df_Data_Lineage_Constraints_Map_6))
#print("...3.3")
### Restructure Constraints Sheet
thisConstraintRow=1
df_Data_Lineage_Constraints_Map_Modified=NA
thisConstraint=NA
selected_keys=NA
#print(nrow(df_Data_Lineage_Constraints_Map_6))
for(thisConstraintRow in 1:nrow(df_Data_Lineage_Constraints_Map_6)){
print(thisConstraintRow)
thisConstraint = df_Data_Lineage_Constraints_Map_6[thisConstraintRow,]
subset_all_layer_keys = thisConstraint [,vars_select(names(thisConstraint), contains("Keys"))]
subset_all_layer_keys =subset_all_layer_keys[,names(subset_all_layer_keys[, colSums(is.na(subset_all_layer_keys))==0])] selected_keys = as.vector(names(subset_all_layer_keys))
df_Data_Lineage_Constraints_Map_Modified = rbind(df_Data_Lineage_Constraints_Map_Modified,
separate_rows(thisConstraint,all_of(selected_keys),sep=','))
}#Constraint loop
#print("...3.4")
#print(selected_keys)
df_Data_Lineage_Constraints_Map_Modified = df_Data_Lineage_Constraints_Map_Modified[
(rowSums(is.na(df_Data_Lineage_Constraints_Map_Modified)) !=
ncol(df_Data_Lineage_Constraints_Map_Modified)),]
df_Data_Lineage_Constraints_Map_Modified = as.data.frame(df_Data_Lineage_Constraints_Map_Modified) #names(df_Data_Lineage_Constraints_Map_Modified)
#names(df_Data_Lineage_Constraints_Map_6)=c(original_column_names_of_constraint_sheet, "Constraint_RecordID")
#names(df_Data_Lineage_Constraints_Map_Modified)=c(original_column_names_of_constraint_sheet,"Constraint_RecordID"
# # print(names(df_Data_Lineage_Constraints_Map_6))
# # print(names(df_Data_Lineage_Constraints_Map_Modified))
#print("...4")
#print("#######...2")
#check in DL sheet, which layer doesn't have field level conditions a.k.a reference tables for getting invidual columns dl_cond_no_cond_layers = get_Layers_With_Or_Without_Conditions(df_Data_Lineage_5)
#print("...5")
dl_no_cond_layers = dl_cond_no_cond_layers$Layers_NoCond
dl_with_cond_layers = dl_cond_no_cond_layers$Layers_Cond #print("...6")
#print("#######...2.1")
join_criteria-as.data.frame(matrix(nrow=NROW(dl_no_cond_layers), ncol= 2))
i=1
#print(NROW(dl_no_cond_layers))
for(i in 1:NROW(dl_no_cond_layers)){
#print("#######...2.2")
#print(i)
join_criteria [i,1]=vars_select(names(vars_select(names(df_Data_Lineage_5), tidyselect::contains("Table"))), tidyselect::contains
(toString(dl_no_cond_layers[i])))
#print(join_criteria [i,1])
#print(names(df_Data_Lineage_5))
join_criteria [i,2]=vars_select(names(vars_select(names(df_Data_Lineage_Constraints_Map_6), tidyselect::contains("Table"))),ti
dyselect::contains(toString(dl_no_cond_layers[i])))
}
#print(names(df_Data_Lineage_Constraints_Map_6))
#print(join_criteria [i,2])
#print("...7")
#print("#######...3.....possible bug in next line, if cond layers are more than 1") if(length(dl_with_cond_layers)>0){
df_Data_Lineage_Constraints_Map_Temp = df_Data_Lineage_Constraints_Map_6[,-
(grep(dl_with_cond_layers,colnames(df_Data_Lineage_Constraints_Map_6)))]
}else{
}
df_Data_Lineage_Constraints_Map_Temp = df_Data_Lineage_Constraints_Map_6
#df_Data_Lineage_Constraints_Map_Temp = as.data.frame(df_Data_Lineage_Constraints_Map_Temp)
#print("...8")
join_criteria [3,1] = 'Remark'
join_criteria [3,2] = 'Remark' xname = join_criteria[,1] yname = join_criteria [,2] #names(df_Data_Lineage_5)
#names(df_Data_Lineage_Constraints_Map_Temp)
# myVar = setNames(nm = xname, yname)
### print(myVar)
# inner_join(df_Data_Lineage_5, df_Data_Lineage_Constraints_Map_Temp,by=myVar) dl_dlc_matching_Tables
innerJoin_MultipleCriteria(df_Data_Lineage_5,
df_Data_Lineage_Constraints_Map_Temp,xname,yname)
dl_dic_matching_Tables= dl_dic_matching_Tables %>% arrange(Constraint_RecordID) #print(nrow(dl_dic_matching_Tables))
#print("...9")
eachLayer=NA
eachLayerName=NA
eachLayerSEQ=NA
columnNames_ThisLayer=NA
thisLayer_TableName=NA
unique_Tables=NA
eachLayer_DataSourceType=NA
eachLayer DataSource Provider=NA
rl=1
#print("#######...7")
#print(nrow(df_Conn_Validation_In_Scope))
first_layer_name = toString(df_Conn_Validation_In_Scope$Layer [df_Conn_Validation_In_Scope$Layer_SEQ==1]) resultFileName_LayerName = toString(unique(df_Conn_Validation_In_Scope$DERIVE_RESULTFILENAME_LAYER)) #print("#######...8")
df_SQL_Statements_All_Layers = df_Data_Lineage_Constraints_Map_6
#print("...10")
for(rL in 1:nrow(df_Conn_Validation_In_Scope)){
#rL=3
#print("#######...9")
eachLayer = df_Conn_Validation_In_Scope[rL,]
#print(eachLayer)
eachLayerName=toString(each Layer$Layer)
eachLayerSEQ= each Layer$Layer_SEQ
eachLayer_DataSourceType = toString(each Layer$ODBC_Prefix)
eachLayer_DataSource Provider = toString(each Layer$ODBC_Provider)
if(each LayerSEQ==0 | | str_detect(str_to_upper(each LayerName), "NATIVE")){ print(eachLayerName)
print("Skipping to next layer !!!")
next }
#print("...11")
# FROM Data Lineage Sheet (dl) & FROM Data Lineage Constraint Sheet (dlc) - To get column names for a LAYER dl_columnNames_ThisLayer = names(vars_select(names(dl_dlc_matching_Tables), contains(each LayerName,ignore.case
TRUE)))
dlc_columnNames_ThisLayer = names(vars_select(names(df_Data_Lineage_Constraints_Map_Modified), contains(each LayerName,ignore.case = TRUE)))
-
# FROM Data Lineage Sheet (dl) & FROM Data Lineage Constraint Sheet (dlc) - To get mapped table & attributes for a LAYER #dl_MappedData_This Layer = as.data.frame(unique (df_Data_Lineage_5[,dl_columnNames_ThisLayer])) dl_MappedData_ThisLayer =
as.data.frame(unique (dl_dlc_matching_Tables[,c(dl_columnNames_ThisLayer, "Constraint_RecordID", "Remark")]))
#print(paste("....DLC Names", names(dl_Mapped Data_This Layer),sep="_"))
dlc_MappedData_ThisLayer =
as.data.frame(unique(df_Data_Lineage_Constraints_Map_Modified [,c(dlc_columnNames_This Layer, "Constraint_RecordID")]))
#print(paste("....DLC Names", names(dic_Mapped Data_ThisLayer),sep="_"))
# FROM Data Lineage Sheet (dl) & FROM Data Lineage Constraint Sheet (dlc) - To get table columnname for a LAYER thisLayer_dl_TableColName = vars_select(dl_columnNames_This Layer,contains("Table", ignore.case = TRUE)) thisLayer_dlc_TableColName = vars_select(dlc_columnNames_ThisLayer,contains("Table", ignore.case = TRUE)) # FROM Data Lineage Sheet (dl) & FROM Data Lineage Constraint Sheet (dlc) - To get schema name for a LAYER thisLayer_dl_SchemaColName = vars_select(dl_columnNames_ThisLayer,contains("Schema", ignore.case = TRUE)) thisLayer_dlc_SchemaColName = vars_select(dlc_column Names_This Layer,contains("Schema", ignore.case = TRUE)) # FROM Data Lineage Sheet (dl) & FROM Data Lineage Constraint Sheet (dlc) - To get Attribute columnname for a LAYER thisLayer_dl_AttributeColName = vars_select(dl_columnNames_ThisLayer,contains("Attribute", ignore.case TRUE)) thisLayer_dlc_AttributeColName = vars_select(dic_columnNames_This Layer, contains("Keys", ignore.case = TRUE)) #print(thisLayer_dlc_AttributeColName)
*
thisLayer_dlc_CdcCriteria ColName = vars_select(dic_columnNames_This Layer,contains("CDC",ignore.case = TRUE)) thisLayer_dl_TransformationColName = vars_select(dl_columnNames_ThisLayer,contains("Transformation", ignore.case
TRUE))
thisLayer_dl_ReferenceIDColName = vars_select(dl_columnNames_This Layer,contains("Reference", ignore.case = TRUE))
dl_dlc_this_TableName=NA
dlc_this_KeysName=NA SQL_Select_Columns=NA dl_dlc_row =1
# for each unique constraint table find out the primary/composite keys and fetch the data
dl_dlc_Unique_TableNames = dl_dlc_matching_Tables[,c(thisLayer_dl_TableColName,"Constraint_RecordID", "Remark")]
%>% distinct(.)
#print("...12")
#print(nrow(dl_dlc_Unique_TableNames))
#print(class(as.list(unique (dl_dlc_Unique_TableNames[,"Constraint_RecordID"]))))
for(dl_dlc_row in dl_dlc_Unique_TableNames$Constraint_RecordID){
#dl_dlc_row=5
#print(dl_dlc_row)
dl_dlc_this_const_ID = dl_dlc_row
dlc_this_TableName = filter(dlc_Mapped Data_ThisLayer,
dlc_MappedData_ThisLayer[,"Constraint_RecordID"]==dl_dlc_this_const_ID ) %>%
select(all_of(thisLayer_dlc_TableColName))
dlc_this_TableName = unique(dlc_this_TableName)
dlc_this_SchemaName = filter(dic_Mapped Data_ThisLayer,
dlc_MappedData_This Layer[,"Constraint_RecordID"]==dl_dlc_this_const_ID) %>%
select(all_of(thisLayer_dlc_SchemaColName))
dlc_this_SchemaName = unique(dlc_this_SchemaName) #print("...12.1")
dlc_this_CdcCriteria = filter(dlc_Mapped Data_ThisLayer,
dlc_MappedData_This Layer[,"Constraint_RecordID"]==dl_dlc_this_const_ID ) %>%
select(all_of(thisLayer_dlc_CdcCriteriaColName))
dlc_this_CdcCriteria = toString(unique(dlc_this_CdcCriteria [1,1]))
dlc_this_KeysName = filter(dlc_Mapped Data_ThisLayer,
dlc_MappedData_ThisLayer[,"Constraint_RecordID"]==dl_dlc_this_const_ID ) %>%
select(all_of(thisLayer_dlc_AttributeColName))
#print(dlc_this_KeysName)
#print(toString(dlc_this_KeysName[,vars_select(names(dlc_this_KeysName),contains("Keys"))]))
dlc_this_KeysName = toString(dlc_this_KeysName[,vars_select(names(dlc_this_KeysName), contains("Keys"))]) #print(dlc_this_KeysName)
#print("...12.2")
print(paste(GP_DATALOAD_TYPE,thisLayer_dic_CdcCriteria ColName,dic_this_CdcCriteria, GP_LAST_DAY1_INCREMENTAL_DAT
E,sep="_"))
if(GP_DATALOAD_TYPE=="INCREMENTAL"){
#print("...12.3")
dlc_this_CdcCriteria =
str_replace(dlc_this_CdcCriteria,"GP_PREV_TEST_EXEC_TIME",GP_LAST_DAY1_INCREMENTAL_DATE)
}else{
#print("...12.4")
dic_this_CdcCriteria =
ifelse(is.na(GP_LAST_DAYO_CUTOFF_DATE), dlc_this_CdcCriteria, str_replace(dlc_this_CdcCriteria,"GP_PREV_TEST_EXEC_TIME ",GP_LAST_DAYO_CUTOFF_DATE))
}
#print("...12.5")
## print(paste(dlc_this_SchemaName[1,1], dlc_this_TableName[1,1],sep="|")) this_Remark filter(dl_dlc_Unique_TableNames,
=
dl_dlc_Unique_TableNames[,"Constraint_RecordID"]==dl_dlc_this_const_ID) %>%
select("Remark")
=
this_Remark unique(this_Remark)
## print(paste("10.......remark=",this_Remark, "....in layer=",each LayerName, "...result file layer=",resultFileName_LayerName,sep=""))
if(each LayerName==resultFileName_LayerName){
=
if(grepl("\\.csv", dlc_this_TableName[1,1], ignore.case TRUE)){
if(!is.na(this_Remark$Remark)){
resultFileName = paste("RESULT", this_Remark$Remark,dlc_this_TableName[1,1],sep="_")
}else{
}
resultFileName = paste("RESULT", dlc_this_TableName[1,1],sep="_")
}else{
if(!is.na(this_Remark$Remark)){
resultFileName = paste("RESULT",this_Remark$Remark,dic_this_TableName[1,1],sep="_") resultFileName = paste (resultFileName," .csv",sep="")
}else{
}
resultFileName = paste("RESULT_",dlc_this_TableName[1,1],".csv",sep="")
}
df_SQL_Statements_All_Layers[dl_dlc_row,"ResultFileName"]=resultFileName
}# When current layer matches with global parameter result layer name
#print("...12.6")
# fetch corresponding attributes of 1st - layer to create column alias for other layers
dl_column_names_Insubset_For_1stLayer = filter(dl_dlc_matching_Tables, Constraint_RecordID==dl_dlc_this_const_ID)
%>%
select(vars_select(names(dl_dlc_matching_Tables),contains(first_layer_name))) %>%
select(vars_select(names(.), contains("Attribute")))
# dl_column_names_Transformation_Insubset_For_1stLayer =
filter(dl_dlc_matching_Tables, Constraint_RecordID==dl_dlc_this_const_ID) %>%
# select(vars_select(names(dl_dlc_matching_Tables), contains(first_layer_name))) %>%
# select(vars_select(names(.),contains("Transformation")))
dl_column_names_Insubset_For_1stLayer = as.data.frame(dl_column_names_Insubset_For_1stLayer)
#dl_column_names_Transformation_Insubset_For_1stLayer = as.data.frame(dl_column_names_Transformation_Insubset_For_1stLayer)
if(eachLayerName==first_layer_name){
dl_column_names_Insubset_For_1stLayer[,1]=paste(" ")
}else{
if(eachLayer_DataSourceType=="file:"){
dl_column_names_Insubset_For_1st Layer[,1]=paste(" AS ", dl_column_names_Insubset_For_1stLayer[,1])
}else{
dl_column_names_Insubset_For_1stLayer[,1]=paste(" AS ", dl_column_names_Insubset_For_1stLayer[,1])
}
}
# now get relevant columns from data lineage
row_DL_Attr = 1
previous_reference_id=0
SQL_Select_Columns=""
referenceSQL=NA
dl_Subset_For_GivenTable = filter(dl_Mapped Data_ThisLayer,
#print("...12.7")
dl_MappedData_This Layer[,"Constraint_RecordID"]==dl_dlc_this_const_ID)
for(row_DL_Attr in 1:nrow(dl_Subset_For_GivenTable)){
#print("...12.8")
dl_Attr_Condition = dl_Subset_For_GivenTable[row_DL_Attr,
names(vars_select(names(dl_Subset_For_GivenTable), contains("Condition")))]
dl_Reference_ID= dl_Subset_For_GivenTable[row_DL_Attr,
names(vars_select(names(dl_Subset_For_GivenTable), contains("Reference")))]
dl_table_name_Insubset = dl_Subset_For_GivenTable[row_DL_Attr, this Layer_dl_TableColName] #print("...12.9")
dl_column_name_Insubset = dl_Subset_For_GivenTable[row_DL_Attr,thisLayer_dl_AttributeColName] #### $$$$$$$$$$$$$$$$$$ #########
## Transformation column, right place to parse for XML or JSON or FWD file functions dl_column_name_Transformation_Insubset =
dl_Subset_For_GivenTable[row_DL_Attr,thisLayer_dl_TransformationColName]
#print(paste0(row_DL_Attr, "----",each Layer_DataSource Provider, "-----",dl_table_name_Insubset,"---- ',dl_column_name_Transformation_Insubset,"-----",dl_column_name_Insubset))
if(!is.na(dl_column_name_Transformation_Insubset)){
}
m_paramters_list = list("m_attr_id"=row_DL_Attr,
"m_db_provider"-each Layer_DataSource Provider,
"m_schema"-dlic_this_SchemaName,
"m_table"=dl_table_name_Insubset,
"m_attr_name"-dl_column_name_Insubset,
"m_attr_transformation"-dl_column_name_Transformation_Insubset,
"m_input_file_folder_path"=BUCKET_INPUT_FOLDER,
"m_output_file_folder_path"-BUCKET_FOLDER,
"m_google_sdk_bin_path"=GOOGLE_SDK_BIN_PATH)
dl_column_name_Transformation_Insubset = parseTransformationColumn(m_paramters_list)
#### $$$$$$$$$$$$$$$$$$ ###
#print("...12.10")
dl_column_name_Insubset_For_1stLayer = dl_column_names_Insubset_For_1stLayer[row_DL_Attr,1]
=
#dl_column_name_Transformation_Insubset_For_1stLayer = dl_column_names_Transformation_Insubset_For_1stLayer[row_DL_Attr,1]
#print("...12.11")
if(!is.na(dl_table_name_Insubset)){ if(eachLayer_DataSourceType=="file:"){
dl_table_name_Insubset = paste0("`",dl_table_name_Insubset,"`")
if(!is.na(dl_Attr_Condition)){
}
dl_Attr_Condition = fileSourceColumnCleanUp(dl_Attr_Condition,"=")
if(is.na(dl_Attr_Condition)){
## without condition
if(grepl("\\+",dl_column_name_Insubset)){
dl_column_name_Insubset = fileSourceColumnClean Up(dl_column_name_Insubset,"+") SQL_Select_Columns =
paste (SQL_Select_Columns,dl_column_name_Insubset,dl_column_name_Insubset_For_1st Layer,",",sep="")
}else{
),"both")
if(str_detect(dl_column_name_Insubset,""")){
SQL_Select_Columns = paste (SQL_Select_Columns,
dl_column_name_Insubset,dl_column_name_Insubset_For_1st Layer,",",sep="")
}else{ ## if file: columnName as 1stColumeName
}
# check if "AS ALIAS" COLUMNNAME REQUIRED or not
1 1111
as_temp_col = str_trim(str_replace(str_to_lower(toString(dl_column_name_Insubset_For_1st Layer)),' as ','
#print(paste0(as_temp_col,"=",str_to_lower(dl_column_name_Insubset)))
if(as_temp_col== str_to_lower(dl_column_name_Insubset)){ SQL_Select_Columns = paste(SQL_Select_Columns,"`",
#
dl_column_name_Insubset,' ",",",sep="")
}else{ # "AS ALIAS" COLUMNNAME REQUIRED
}
SQL_Select_Columns = paste (SQL_Select_Columns,"`",
dl_column_name_Insubset,"",dl_column_name_Insubset_For_1st Layer,",",sep="")
# SQL_Select_Columns = paste (SQL_Select_Columns,'
#
} }else{
}
## with condition
dl_column_name_Insubset,"`", dl_column_name_Insubset_For_1st Layer,",",sep=""")
if(grepl("\\+",dl_column_name_Insubset)){
}
dl_column_name_Insubset = fileSourceColumnCleanUp(dl_column_name_Insubset,"+")
SQL_Select_Columns = paste(SQL_Select_Columns," (SELECT", dl_column_name_Insubset, "FROM ",dl_table_name_Insubset,
"WHERE ",dl_Attr_Condition,") ",dl_column_name_Insubset_For_1stLayer,",",sep="")
## FILE Transformation Logic is not coded as in RDBMS
}else{# RDBMS Conditions
## Schema name is available or not available
if(!is.na(dlc_this_SchemaName)){
dl_table_name_Insubset=paste0(dlc_this_SchemaName,".",dl_table_name_Insubset)
}## Schema name
## Reference Columnn ID is blank or not
if(!is.na(dl_Reference_ID)){
## Previous run REFID != CUR_REFID Condition
if(previous_reference_id != dl_Reference_ID){
referenceSQL = filter(df_Data_Lineage_Reference_Data_Map_7,Layer==eachLayerName & Reference_ID==dl_Reference_ID) %>%
select(Reference_Condition)
referenceSQL = referenceSQL[1,1]
#referenceSQL = paste0(referenceSQL," A_",dl_Reference_ID,",")
previous_reference_id = dl_Reference_ID
}## Previous run REFID != CUR_REFID Condition
#SQL_Select_Columns = paste(SQL_Select_Columns, dl_column_name_Insubset,dl_column_name_Insubset_For_1st Layer,",",sep="")
}## Reference ID is blank or not condition
## When Transformation is available or not available if(trimws(dl_column_name_Insubset_For_1stLayer)=="""){ if(is.na(dl_column_name_Transformation_Insubset)){ SQL_Select_Columns = paste(SQL_Select_Columns,
dl_column_name_Insubset,dl_column_name_Insubset_For_1st Layer,",",sep="")
#print(paste0(". **** ....NA",dl_column_name_Insubset,dl_column_name_Insubset_For_1stLayer))
}else{
}
if(str_detect(dl_column_name_Transformation_Insubset," ")){
}
dl_column_name_Transformation_Insubset=paste("\"",dl_column_name_Transformation_Insubset,"\"")
#print(paste0(".
........
****
....",dl_column_name_Transformation_Insubset," AS ",dl_column_name_Insubset))
SQL_Select_Columns = paste(SQL_Select_Columns,dl_column_name_Transformation_Insubset, AS ",dl_column_name_Insubset, ",",sep="")
}else{
if(is.na(dl_column_name_Transformation_Insubset)){
#print("######")
SQL_Select_Columns = paste(SQL_Select_Columns,
dl_column_name_Insubset,dl_column_name_Insubset_For_1st Layer,",",sep="")
}else{
}
if(str_detect(dl_column_name_Transformation_Insubset," ")){
}
dl_column_name_Transformation_Insubset=paste("\"",dl_column_name_Transformation_Insubset,"\"")
#print(paste0("1111111",dl_column_name_Transformation_Insubset,dl_column_name_Insubset_For_1stLayer))
SQL_Select_Columns = paste (SQL_Select_Columns, dl_column_name_Transformation_Insubset,
dl_column_name_Insubset_For_1stLayer,",",sep="")
}##Alias column name blank or not Condition
# When Criteria to fetch a columns is defined
if(!is.na(dl_Attr_Condition)){
SQL_Select_Columns = str_replace(SQL_Select_Columns,dl_column_name_Insubset,
paste(" (",
11
SELECT", dl_column_name_Insubset,
}
"FROM ", dl_table_name_Insubset,
"WHERE ",dl_Attr_Condition, ")",sep=""))
}## FILE or RDBMS Condition
}else{
SQL_Select_Columns=""
referenceSQL=NA
} ## if or not Table name is empty, then exit
}## End of FOR loop for each column in a table
SQL Statement="""
if(SQL_Select_Columns != ""){ if(eachLayer_DataSourceType=="file:"){
}
SQL_Select_Columns
= str_replace_all(SQL_Select_Columns,"\\.","#")
SQL_Select_Columns = str_replace_all(SQL_Select_Columns," #csv","\\.csv")
SQL_Select_Columns = strtrim(str_trim(SQL_Select_Columns), str_length(str_trim(SQL_Select_Columns))-1) ## if referenceSQL is NA or not
if(!is.na(referenceSQL)){
# here parse referenceSQL for if TDV for getting TDV based tables table name
SQL_Statement = str_replace(referenceSQL,"<SELECT>", paste(" SELECT", SQL_Select_Columns,sep="")) SQL_Statement = str_replace(SQL_Statement,"<FROM>"," FROM ")
}else{
SQL_Statement=paste("SELECT",SQL_Select_Columns,sep="")
#SQL_Statement = strtrim(str_trim(SQL_Statement), str_length(str_trim(SQL_Statement))-1) if(eachLayer_DataSourceType=="file:"){
if(str_detect(dlc_this_TableName[1,1],"\\.csv")){
SQL_Statement = paste(SQL_Statement, " FROM ",dlc_this_TableName[1,1],"`",sep="")
}else{
}
SQL_Statement = paste (SQL_Statement, " FROM "", dic_this_TableName[1,1], ".csv",sep=""")
}else{
m_strTableName =
ifelse(str_detect(dlc_this_TableName[1,1],".csv"), paste0("\"",dlc_this_TableName[1,1],"\""), dlc_this_TableName[1,1]) if(is.na(dlc_this_SchemaName)){
SQL_Statement = paste(SQL_Statement, " FROM ",m_strTableName,"",sep="")
}else{
SQL_Statement = paste(SQL_Statement, " FROM ",dlc_this_SchemaName[1,1],".",m_strTableName,"",sep="")
}
}
}## if referenceSQL is NA or not
#print("...12.15")
#print(dlc_this_CdcCriteria)
if(!is.null(dlc_this_CdcCriteria) && !is.na(dlc_this_CdcCriteria) && dlc_this_CdcCriteria!="NA"){
#print("...12.7")
#print(dlc_this_KeysName)
if(!is.null(dlc_this_KeysName) && !is.na(dlc_this_KeysName) && dlc_this_KeysName!="NA"){
#print(dlc_this_KeysName)
#print("...12.8")
dlc_this_CdcCriteriaAndOrder=""
if(str_detect(dlc_this_CdcCriteria,"LIMIT")){
LIMIT "))
dlc_this_CdcCriteria AndOrder = str_replace(dic_this_CdcCriteria," LIMIT",paste0(" ORDER BY ", dlc_this_KeysName,"
}else if(str_detect(dic_this_CdcCriteria, "FETCH")){
dlc_this_CdcCriteria AndOrder = str_replace(dlc_this_CdcCriteria,"FETCH", paste0(" ORDER BY ", dlc_this_KeysName,"
FETCH ")) }else{
}
dlc_this_CdcCriteria AndOrder=""
if(dlc_this_CdcCriteria AndOrder==""){
SQL_Statement = paste (SQL_Statement," WHERE ", dlc_this_CdcCriteria,sep="")
}else{
}
SQL_Statement = paste(SQL_Statement," WHERE ",dlc_this_CdcCriteriaAndOrder,sep="")
}else{
}
SQL_Statement = paste (SQL_Statement," WHERE ",dlc_this_CdcCriteria,sep="" )
#print(SQL_Statement)
}
#print("...12.16")
}else{
SQL_Statement=NA
}
#print("...16")
df_SQL_Statements_All_Layers[dl_dic_row,paste(each LayerName, "SQLS",sep="_")]= SQL_Statement
# find out if file or odbc? accordingly call do connect and fire sql statements
}# for loop: DLC Table
}# for loop: Layer row #print("...17")
return(list(nm_df_Conn_Validation_In_Scope= df_Conn_Validation_In_Scope, nm_df_SQL_Statements_All_Layers =df_SQL_Statements_All_Layers ))
# executeCode functions Error
##
***
****
buildConnectionString = function(this_DBConnection_Details){ connectionString=NA
#print(this_DBConnection_Details$ODBC_DSN_Name)
if(! is.na(this_DBConnection_Details$ODBC_DSN_Name)){
#print(this_DBConnection_Details$ODBC_UID)
if(! is.na(this_DBConnection_Details$ODBC_UID)){
connectionString = paste("DSN=",this_DBConnection_Details$ODBC_DSN_Name,";",
}else{
}
"UID=",this_DBConnection_Details$ODBC_UID,";", "PWD=",this_DBConnection_Details$ODBC_PWD,sep="")
connectionString = paste("DSN=",this_DBConnection_Details$ODBC_DSN_Name,";",sep=""")
}else{
if(this_DBConnection_Details$ODBC_Prefix=="jdbc:"){ if(this_DBConnection_Details$ODBC_Provider=="TD"){
connectionString = paste0(this_DBConnection_Details$ODBC_Prefix,
"teradata://",
this_DBConnection_Details $ODBC_Host_Name)
}else if(this_DBConnection_Details$ODBC_Provider=="TD-LDAP"){
connectionString = paste0(this_DBConnection_Details$ODBC_Prefix, "teradata://",
this_DBConnection_Details$ODBC_Host_Name,"/LOGMECH=LDAP")
}else if(this_DBConnection_Details$ODBC_Provider=="TDV"){ connectionString = paste0(this_DBConnection_Details $ODBC_Prefix,
"compositesw:dbapi@",
this_DBConnection_Details$ODBC_Host_Name,#":",
#this_DBConnection_Details$ODBC_Port,"?",
"domain=composite",#"&",
#"dataSource=",this_DBConnection_Details$ODBC_Database
)
}else if(this_DBConnection_Details$ODBC_Provider=="PostgreSQL"){
connectionString = paste0(this_DBConnection_Details$ODBC_Prefix, "postgresql://",
this_DBConnection_Details$ODBC_Host_Name,":", this_DBConnection_Details$ODBC_Port,"/", this_DBConnection_Details$ODBC_Database
)
#print(connectionString)
}else if(this_DBConnection_Details$ODBC_Provider=="DB2ISERIES"){ connectionString = paste0(this_DBConnection_Details $ODBC_Prefix, "as400://",
this_DBConnection_Details$ODBC_Host_Name)
}else if(this_DBConnection_Details$ODBC_Provider=="CDH_HIVE"){
connectionString = paste0(this_DBConnection_Details$ODBC_Prefix, "hive2://",
this_DBConnection_Details$ODBC_Host_Name)
}else if(this_DBConnection_Details$ODBC_Provider=="FILE"){
connectionString = paste0(this_DBConnection_Details$ODBC_Prefix,
} }else{
"xbib:csv:/",
this_DBConnection_Details$ODBC_Host_Name)
if(this_DBConnection_Details$ODBC_Provider=="Oracle"){
connectionString = paste("Driver=",this_DBConnection_Details$ODBC_Driver,";",
#"SERVER=",this_DBConnection_Details$ODBC_Host_Name, ":",this_DBConnection_Details$ODBC_Port,";", "UID=",this_DBConnection_Details$ODBC_UID,";", "PWD=",this_DBConnection_Details$ODBC_PWD,";",
"DBQ=",this_DBConnection_Details$ODBC_Service_Name,sep="")
}else if(this_DBConnection_Details$ODBC_Provider=="PostgreSQL"){
connectionString = paste("Driver=",this_DBConnection_Details$ODBC_Driver, ";", "Server=",this_DBConnection_Details$ODBC_Host_Name,";", "Port=",this_DBConnection_Details$ODBC_Port,";", "Uid=",this_DBConnection_Details$ODBC_UID, ";",
}else{
"Pwd=",this_DBConnection_Details$ODBC_PWD,";",
"Database=",this_DBConnection_Details$ODBC_Database,sep="")
connectionString = paste("Driver=",this_DBConnection_Details$ODBC Driver,";",
#"SERVER=",this_DBConnection_Details$ODBC_Host_Name, ":",this_DBConnection_Details $ODBC_Port,";",
"UID=",this_DBConnection_Details$ODBC_UID, ";",
"PWD=",this_DBConnection_Details$ODBC_PWD,";",
"DBQ=",this_DBConnection_Details$ODBC_Service_Name,sep="")
}
}
}
#print(connectionString)
return(connectionString)
}# DB buildConnectionString end - 32 bit driver
## ***
****
connectDB_GetResult = function(connectionString,m_SQLQuery,this_DBConnection_Details){
data=NA
conn=NA
drv=NA
m_action=ifelse(stri_startswith_fixed (m_SQLQuery, "CREATE"),"WRITE", "READ")
m_action=ifelse(stri_startswith_fixed (m_SQLQuery, "DROP"),"WRITE", "READ")
#print(m_action)
#print(m_SQLQuery)
#print(connectionString)
if(this_DBConnection_Details$ODBC_Prefix=="jdbc:"){ ## JDBC connection
#m_driver_prefix = this_DBConnection_Details$Driver_Prefix m_driver_provider = this_DBConnection_Details$ODBC_Provider print(m_driver_provider)
m_driver_folder = paste0(CURRENT_FOLDER,"/jdbc/",m_driver_provider) m_file_paths = list.files(m_driver_folder,full.names = TRUE)
m_classPath=""
#print(.jclassPath())
for(file in m_file_paths){
#print(file)
}
#.jaddClassPath(file)
m_classPath = paste0(m_classPath, file, ";")
#print(m_classPath)
if(this_DBConnection_Details $ODBC_Provider=="TD" }{
drv = RJDBC::JDBC("com.teradata.jdbc.Tera Driver",m_classPath)
conn = RJDBC::dbConnect(drv, connectionString,
port=this_DBConnection_Details$ODBC_Port,
dbname = this_DBConnection_Details$ODBC_Database,
user=this_DBConnection_Details$ODBC_UID, password=this_DBConnection_Details$ODBC_PWD)
}else if(this_DBConnection_Details$ODBC_Provider=="TD-LDAP" ){ drv = RJDBC::JDBC("com.teradata.jdbc.Tera Driver",m_classPath) conn = RJDBC::dbConnect(drv, connectionString,
port=this_DBConnection_Details$ODBC_Port,
dbname = this_DBConnection_Details$ODBC_Database,
user=td_ldap_user,#this_DBConnection_Details $ODBC_UID,
password=td_ldap_pwd) #this_DBConnection_Details$ODBC_PWD)
}else if(this_DBConnection_Details$ODBC_Provider=="TDV"){
)
#library(RJDBC)
#tdv_driverPath = "C:\\01-Data\\00-Plans\\002_TIBCO\\Archive\\TIB_tdv_drivers_8.3.0_all\\apps\\jdbc\\lib\\csjdbc8.jar" drv = RJDBC::JDBC("cs.jdbc.driver.Composite Driver", m_classPath)
conn = RJDBC::dbConnect(drv, connectionString,
port=this_DBConnection_Details$ODBC_Port,
dbname=this_DBConnection_Details$ODBC_Database, user=this_DBConnection_Details$ODBC_UID,
password=this_DBConnection_Details$ODBC_PWD
}else if(this_DBConnection_Details$ODBC_Provider=="PostgreSQL"){ drv = RJDBC::JDBC("org.postgresql.Driver",m_classPath)
)
conn = RJDBC::dbConnect(drv, connectionString,
#port=this_DBConnection_Details$ODBC_Port,
#dbname=this_DBConnection_Details$ODBC_Database, user=this_DBConnection_Details$ODBC_UID,
password=this_DBConnection_Details$ODBC_PWD
#print("Inside POSTGRESQL..done")
}else if(this_DBConnection_Details$ODBC_Provider=="DB2ISERIES"){
=
drv RJDBC::JDBC("com.ibm.as400.access. AS400JDBC Driver",m_classPath) conn = RJDBC::dbConnect(drv, connectionString,
port=this_DBConnection_Details$ODBC_Port,
)
dbname=this_DBConnection_Details$ODBC_Database, user=this_DBConnection_Details$ODBC_UID, password=this_DBConnection_Details$ODBC_PWD
}else if(this_DBConnection_Details$ODBC_Provider=="CDH_HIVE"){
drv = RJDBC::JDBC("com.cloudera.hive.jdbc.HS2 Driver",m_classPath)
otherJDBCParam=NA
m_otherParam_split-NA
m_transportMode=NA
m_httpPath=NA
m_sslTrustStore=NA
m_trustStorePassword =NA
m_ssl = NA
if(!is.na(this_DBConnection_Details$OtherJDBCParam)){
-
otherJDBCParam = this_DBConnection_Details$OtherJDBCParam m_otherParam_split = unlist(str_split(otherJDBCParam,";"))
m_transportMode = unlist(str_split(m_otherParam_split[str_detect(m_otherParam_split,"transportMode=")],"="))[2] m_httpPath = unlist(str_split(m_otherParam_split[str_detect(m_otherParam_split,"httpPath=")],"="))[2]
m_sslTrustStore = unlist(str_split(m_otherParam_split[str_detect(m_otherParam_split, "sslTrustStore=")],"="))[2] m_trustStorePassword = unlist(str_split(m_otherParam_split[str_detect (m_otherParam_split, "trustStore Password=")],"="))[2]
}
m_ssl = unlist(str_split(m_otherParam_split[str_detect(m_otherParam_split, "ssl=")],"="))[2]
conn = RJDBC::dbConnect(drv, connectionString,
port=this_DBConnection_Details$ODBC_Port,
dbname=this_DBConnection_Details$ODBC_Database, user=this_DBConnection_Details$ODBC_UID,
password=this_DBConnection_Details$ODBC_PWD,
sslTrustStore = ifelse(!is.na(m_sslTrustStore),m_sslTrustStore,""),
trustStorePassword = ifelse(!is.na(m_trustStorePassword),m_trustStorePassword,""),
ssl= ifelse(!is.na(m_ssl),m_ssl,""),
transportMode = ifelse(!is.na(m_transportMode),m_transportMode,""), httpPath = ifelse(!is.na(m_httpPath), m_httpPath,"")
)
}else if(this_DBConnection_Details$ODBC_Provider=="FILE"){
drv = RJDBC::JDBC("org.xbib.jdbc.csv.Csv Driver",m_classPath) conn = RJDBC::dbConnect(drv, connectionString
#,
# port=this_DBConnection_Details$ODBC_Port,
# dbname=this_DBConnection_Details$ODBC_Database, #user=this_DBConnection_Details$ODBC_UID,
# password=this_DBConnection_Details$ODBC_PWD
)
}
if(m_action=="READ"){
=
data RJDBC::dbGetQuery(conn, m_SQLQuery) }else{
}
RJDBC::dbSendUpdate(conn, m_SQLQuery)
data = "No Data"
RJDBC::dbDisconnect(conn)
}else{ ## ODBC COnnnection
if(str_detect(this_DBConnection_Details$ODBC_Driver_Type,"32")){
data = MySQLQuery For32BitODBCDriver(connectionString,m_SQLQuery) }else{
conn = RODBC::odbc DriverConnect(connectionString)
data = RODBC::sqlQuery(conn,query=m_SQLQuery,errors = TRUE)
RODBC::odbcCloseAll()
}
}
return(data)
}
##
*
# data = MYSQLQuery For32 BitODBC Driver("DSN=CMB_DRefinery_OutputDirFolder;","SELECT
SubHeader#RelationshipCustomerID` AS party_identifier, 'O' AS party_type_code FROM `Party_ACCT.csv` WHERE SubHeader#RelationshipCustomerID = '21"")
MySQLQueryFor32BitODBC Driver = function(connectionString,m_SQLQuery) {
}
tempdir=gsub('\\\\','/',tempdir())
txt = paste("if (!'RODBC' %in% installed.packages()) install.packages('RODBC', quiet =TRUE)","\n",
"suppressMessages(suppressWarnings(library(RODBC, quietly=TRUE)))\n\n",
"channel = RODBC::odbc DriverConnect("",connectionString,")\n",
"data=sqlQuery(channel,\"",m_SQLQuery,"\",errors = TRUE,as.is=TRUE)\n",
"#print(data)\n",
"save(data,file=paste("", tempdir,"", 'temp RODBCquery.Rdata',sep='/'), ascii=TRUE)\n",
"close(channel)",sep="")
writeLines(txt,con=paste(tempdir, 'RODBCscripttemp.r',sep='/')->tempscript)
system(paste0(Sys.getenv("R_HOME"), "/bin/i386/Rscript.exe ", tempscript))
tt=get(load(paste(tempdir, 'tempRODBCquery.Rdata',sep='/')))
return(tt)
}
##
*
**
fileSourceColumnClean Up = function(str_concatenated_colnames,char_delimiter){ my_ds = as.data.frame(str_concatenated_colnames) names(my_ds) = c("Col_Name")
my_ds$Col_Name= str_replace_all(my_ds$Col_Name,"\r","") my_ds$Col_Name= str_replace_all(my_ds$Col_Name,"\n","")
my_ds1=separate_rows(my_ds, Col_Name,sep=paste("\\",char_delimiter), convert = TRUE) my_ds1$ID = seq(1:nrow(my_ds1))
my_ds2=filter(my_ds1,grepl("'", Col_Name) ==FALSE & grepl("`", Col_Name)== FALSE) my_ds3-filter(my_ds1,grepl("'", Col_Name)==TRUE | grepl("`", Col_Name)==TRUE)
my_ds2 = my_ds2 %>% mutate (Col_Name=paste("`",Col_Name,"`"))
my_ds1 = union_all(my_ds2,my_ds3) %>% arrange(ID)
my_ds1 = my_ds1 %>% summarise (Col_Name = str_c(Col_Name,collapse = char_delimiter)) %>% ungroup() return(toString(my_ds1$Col_Name))
## **
innerJoin_MultipleCriteria = function(xData,yData,xnames,ynames){
myVar = setNames(nm = xnames, ynames)
## print(myVar)
inner_join(xData,yData,by=myVar) %>%
rename_at(vars(ends_with(".x")),~str_replace(., "\\..$","") ) %>% select_at(vars(-ends_with(".y")))
}
##
get_Layers_With_Or_Without_Conditions = function(dataLineageSheetData){
dl_no_cond_layers ="""
dl_with_cond_layers=""
dl_no_cond_layers-names(which(colSums(is.na(dataLineageSheetData[,names(vars_select(names(dataLineageSheetData), con
tains("Condition")))]))
==nrow(dataLineageSheetData)))
dl_no_cond_layers = str_trim(str_replace(dl_no_cond_layers, "Condition","")) dl_with_cond_layers =
names(which(colSums(is.na(data LineageSheetData[,names(vars_select(names(data LineageSheetData), contains("Condition")))
]))
}
!=nrow(dataLineageSheetData)))
-
dl_with_cond_layers = str_trim(str_replace(dl_with_cond_layers, "Condition",""))
return(list(Layers_NoCond =dl_no_cond_layers, Layers_Cond =dl_with_cond_layers ))
##
**
concat_DF_cols_for_TDV = function(this_layer_DB_Provider,df_data){
m_col=""
m_concatenated_col=""
m_num=1
if(this_layer_DB_Provider=="TDV"){
for(m_col in colnames(df_data)){
#print(m_col)
if(m_num==1){
m_concatenated_col = paste0("NVL(TRIM(CAST(",m_col," AS VARCHAR(2000))), 'NULL')",m_concatenated_col)
}else{
m_concatenated_col = paste0("CONCAT(NVL(TRIM(CAST(",m_col," AS
VARCHAR(2000))), 'NULL'),',m_concatenated_col,")")
}
}
}
#print(m_concatenated_col)
m_num = m_num+1
}else if(this_layer_DB_Provider=="PostgreSQL"){
m_concatenated_col = paste0(colnames(df_data), collapse = ",")
m_concatenated_col = paste0("CONCAT(",m_concatenated_col,")")
}
return(m_concatenated_col)
## **
***
getTDV_RESTAPI = function(apiURL){
#path = "https://cdn-api.co-vin.in/api/v2/appointment/sessions/public/calendarByDistrict? district_id=363&date=21-05-
2021"
}
#path
=
"http://localhost:9400/rest/folder/v1?path=/users/composite/admin/Views/RESULT&summary=true"
#print(m_path)
r=GET(url=apiURL,authenticate("admin", "password", type = "basic"))
print(paste0(apiURL,"---->", status_code(r)))
rm=content(r,as="text", encoding="UTF-8")
#class(rm)
df = as.data.frame(t(as.matrix(fromJSON(rm, flatten = TRUE))))
#class(df)
return(df)
getTDV_RESTAPI = function(requestType,apiURL,reqBody=""){
# requestType="POST"
# apiURL=m_path
# reqBody=m_body
if(requestType=="GET"){
m_response = GET(url=apiURL,
authenticate("admin", "password", type = "basic"))
}else if(requestType=="POST"){ m_response = POST(url=apiURL,
body=reqBody,
authenticate("admin", "password", type = "basic"), add_headers("Content-Type"="application/json","accept"="*/*"))
}else if(requestType=="PUT"){
}else if(requestType=="DELETE"){
m_response = DELETE(url = apiURL,
}else{
}
body=reqBody,
authenticate("admin", "password", type = "basic"),
add_headers("Content-Type"="application/json","accept"="*/*"))
print("Unknown API request type ! Please use GET, PUT, POST or DELETE.")
break;
df=NA
rm=content(m_response,as="text", encoding="UTF-8")
if(status_code(m_response)==201 || status_code(m_response)==200){ #print("successfully reqest fullfilled")
if(rm != ""){
df = as.data.frame(as.matrix(fromJSON(rm, flatten = TRUE)))
}else{
df=""
}
}else{
}
print(paste("Error with status code : ",status_code (m_response),rm))
return(df)
}## Function end
########################
create_TDV_VIEW_FOLDER=function(tdv_parent_path,tdv_folder_name,tdv_rest_base_url){
}
### RECREATE THE FOLDERS and VIEWS
m_path = paste0(tdv_rest_base_url,"/folder/v1") m_body = paste0("[{\"parent Path\":\"",tdv_parent_path,
"\", \"name\":\"",tdv_folder_name,
"\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]")
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## FOLDER CREATED
ifelse(m_result=="",print(paste0("Created folder = ",tdv_parent_path,"/",tdv_folder_name)), print(paste0("Error in Creating foler = ",tdv_parent_path,"/",tdv_folder_name)))
create_AS_IS_VIEW = function(this_layer_DB_Provider,
m_layer,
tdv_parent_path,
tdv_publish_path,
tdv_folder_name,
tdv_CatalogName,
tdv_schema_name,
tdv_table_name,
tdv_query,
tdv_rest_base_url){
tdv_view_name1=paste0(tdv_table_name,"_",m_layer)
newViewPath=""
if(this_layer_DB_Provider=="TDV"){## TDV
print("First Deleting Published View then, original table view, to re-create later..")
m_path = paste0(tdv_rest_base_url,"/link/v1")
tdv_cat_schema =
ifelse(is.na(tdv_CatalogName)==TRUE,tdv_schema_name,paste0("\\\"",tdv_CatalogName,"\\\"/",tdv_schema_name))
## DELETE PUBLISHED
m_body=paste("[{\"path\": \"", paste0(tdv_publish_path,"/",tdv_cat_schema,"/",tdv_view_name1),"\",",
"\"isTable\":true}]")
m_result = getTDV_RESTAPI("DELETE",m_path,m_body)
## DELETE ORIGINAL TABLE VIEW TO RE-CREATE LATER
m_path = paste0(tdv_rest_base_url,"/dataview/v1?ifExists=false")
m_body = paste("[",
"\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1, "\"",
"]")
#print(m_body)
m_result = getTDV_RESTAPI("DELETE",m_path,m_body)
ifelse(m_result =="", print(paste0("Deleted Views From Folder = ",tdv_parent_path,"/",tdv_folder_name)), print(paste("Error in Deleting Views From Folder = ",tdv_parent_path,"/",tdv_folder_name)))
print("Creating original table views...")
newViewPath=""
m_path = paste0(tdv_rest_base_url,"/dataview/v1")
m_body = paste0("[{\"parent Path\":\"", paste0(tdv_parent_path,"/",tdv_folder_name),
"\",\"name\":\"",tdv_view_name1,
"\",\"sql\": \"",tdv_query,
"\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 1st - VIEW CREATED - UNION if(m_result == "")}{
print(paste("Created view = ",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1))
newViewPath=paste(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)
m_path = paste0(tdv_rest_base_url,"/link/v1")
## PUBLISH AGAIN
m_body=paste("[{\"targetPath\": \"", newViewPath,"\",",
"\"isTable\": true,",
"\"path\":\"", paste0(tdv_publish_path,"/",tdv_cat_schema,"/",tdv_view_name1),"\"}]") # #"\"annotation\": \"New Published View\"")
# #"\"ifNotExists\": true", "}]")
#print(m_body)
m_result = getTDV_RESTAPI("POST",m_path,m_body)
ifelse(m_result =="", print("Published"), print("NOT Published"))
}else{
}
print(paste0("Error in Creating foler = ",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1))
}else if(this_layer_DB_Provider=="PostgreSQL"){ ## PostgreSQL
}
## create a table view
#tdv_query = paste0("CREATE OR REPLACE ",tdv_view_name1, " AS ",tdv_query)
return(newViewPath)
}## function end
########################
get_TDV_SQL_RESULT = function(tdv_rest_base_url,tdv_query){
m_result=""
m_body=""
m_path =paste0(tdv_rest_base_url,"/execute/v1/actions/query/invoke")
# if(tdv_result_type=="Summary"){
# tdv_query = paste0("SELECT * FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name9)) # }else{
# ## DETAILED RESULTS
# tdv_query = paste0("SELECT * FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name8))
#}
m_body = paste0("{\"standardSQL\":false,",
"\"query\": \"",tdv_query,
"\",\"system\":\"false"
"\"}") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 9th - VIEW CREATED - TEST SUMMARY ALL SUBSET
#View(m_result)
if(class(m_result)=="data.frame"){
print("Successully Invoked Query")
}else{
#print(m_result)
print("Error Invoking Query")
}
return(m_result)
}
#####################
create_TDV_RESULT_VIEWS =
function(tdv_parent_path,tdv_folder_name,tdv_query,m_tdv_all_col_names,tdv_rest_base_url,tdv_result_type=c("Summary
","Detailed")){
#tdv_pod_name="""
#tdv_env=""
#tdv_table_name=""
tdv_view_name1=paste("UNION_VIEW1")
tdv_view_name2=paste("ONLY_IN_SUBSET_VIEW2")
tdv_view_name3=paste("DUPLICATE_IN_SUBSET_VIEW3")
tdv_view_name4-paste0("ONLY_UNION_DUPLICATE_VIEW4")
tdv_view_name5-paste0("PASS_FAIL_SUBSET_VIEW5")
tdv_view_name6=paste("PASS_SUBSET_VIEW6")
tdv_view_name7-paste0("FAIL_SUBSET_VIEW7")
tdv_view_name8=paste("CONSOLIDATE_VIEW8")
tdv_view_name9-paste0("TEST_SUMMARY_VIEW9")
m_result=""
m_body=""
tdv_all_col_names = m_tdv_all_col_names
if(is.null(tdv_all_col_names)){
tdv_all_col_names=" "
}else{
}
tdv_all_col_names-paste0(" ",tdv_all_col_names,",")
#### DELETE ALL THE DERIVED VIEW AS ASSIGNED ABOVE IN ONE GO m_path = paste0(tdv_rest_base_url, "/dataview/v1?ifExists=false") m_body=paste("[",
"\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name9, "\"" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name8, "\""" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name7, "\"" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name6, "\"" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name5, "\"" "\"""',tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name4, "\""
1
11 11
11 11
11 11
11 11
11 11
1
11 11
"
"\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name3, "\"" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name2, "\"" "\"",tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1, "\"",
"]") #print(m_body)
m_result = getTDV_RESTAPI("DELETE",m_path,m_body)
""
}
"
ifelse(m_result =="",print(paste0("Deleted Views From Folder = ",tdv_parent_path,"/",tdv_folder_name)), print(paste("Error in Deleting Views From Folder = ",tdv_parent_path,"/",tdv_folder_name)))
28
#print("Creating a union 1st view now..")
m_path = paste0(tdv_rest_base_url,"/dataview/v1")
#tdv_query="SELECT
EMPLY_RM_NUM,CUST_PROD_CAT_CDE,CUST_CAT_CDE, EXPN_ANNL_AMT,CUST_ID
GROUP_MEMBR_CDE,HASHSHA(CONCAT(NVL(CAST(GROUP_MEMBR_CDE AS
VARCHAR), 'NULL'), CONCAT(NVL(CAST(CUST_ID AS VARCHAR), 'NULL'),CONCAT(NVL(CAST(EXPN_ANNL_AMT AS VARCHAR), 'NULL'),CONCAT(NVL(CAST(CUST_CAT_CDE AS VARCHAR), 'NULL'), CONCAT(NVL(CAST(CUST_PROD_CAT_CDE AS VARCHAR), 'NULL'), NVL(CAST(EMPLY_RM_NUM AS VARCHAR), 'NULL'))))))) AS MERGE_ALL, HASHSHA(NVL(CAST(CUST_ID AS VARCHAR), 'NULL')) AS MERGE_KEYS,'TD_TARGET' AS LAYER, 1 AS LAYER_SEQ,'NA' AS
RECONCILE_TYPE, 'RESULT_W01SPM_HUB_CUST_PORTF.csv' AS FILE_NAME FROM
/users/composite/admin/GDT_Shared_Services/GDE_SPM/TD_GISD_SPM/FR_GISD_SPM_DLWORK/W01SPM_HUB_CUST_PO RTF UNION ALL SELECT EMPLY_RM_NUM AS EMPLY_RM_NUM,CUST_PROD_CAT_CDE AS CUST_PROD_CAT_CDE,CUST_CAT_CDE AS CUST_CAT_CDE, EXPN_ANNL_AMT AS EXPN_ANNL_AMT,CUST_ID AS CUST_ID,GROUP_MEMBR_CDE AS GROUP_MEMBR_CDE,HASHSHA(CONCAT(NVL(CAST(GROUP_MEMBR_CDE AS VARCHAR), 'NULL'),CONCAT(NVL(CAST(CUST_ID AS VARCHAR), 'NULL'), CONCAT(NVL(CAST(EXPN_ANNL_AMT AS VARCHAR), 'NULL'), CONCAT(NVL(CAST(CUST_CAT_CDE AS VARCHAR), 'NULL'), CONCAT(NVL(CAST(CUST_PROD_CAT_CDE AS VARCHAR), 'NULL'), NVL(CAST(EMPLY_RM_NUM AS VARCHAR), 'NULL'))))))) AS MERGE_ALL, HASHSHA(NVL(CAST(CUST_ID AS VARCHAR), 'NULL')) AS MERGE_KEYS, 'TD_SOURCE' AS LAYER,2 AS LAYER_SEQ,'NA' AS RECONCILE_TYPE, 'RESULT_WO1SPM_HUB_CUST_PORTF.csv' AS FILE_NAME FROM
/users/composite/admin/GDT_Shared_Services/GDE_SPM/TD_GISD_SPM/FR_GISD_SPM_DLWORK/SPM_HUB_CUST_VIEW_T
UNE_LCL"
# Create folder RESULT if ont already existing at /users/composite/admin/Views: POST request with body tdv_view_name1=paste("UNION_VIEW1")
# CREATE UNION VIEWS BASED on OTHER INPUTS LIKE PODNAME, ENV, TABLENAME,"_UNION_VIEW" like PODNAME_ENV_TABLENAME UNION VIEW
-
m_body = paste0("[{\"parent Path\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\":\"",tdv_view_name1,
"\",\"sql\": \"",tdv_query,
"\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
#m_body = "[{\"parent Path\":\"/shared/examples\", \"name\":\"sampleView\", \"sql\":\"SELECT OrderID FROM /shared/examples/ViewOrder\", \"annotation\":\"This view is created using REST api\"}]"
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 1st - VIEW CREATED - UNION
if(m_result ==""){
print("1st view created successfully.")
#print("Creating a 2nd view now..")
m_path =paste0(tdv_rest_base_url,"/dataview/v1") tdv_query = paste("select
",tdv_all_col_names,"FILE_NAME,m_data.LAYER,m_data.MERGE_ALL,m_data.MERGE_KEYS, CONCAT('Onlyln_',m_data.LAYER)
AS RECONCILE_TYPE,m_count AS M_COUNT",
FROM (select count(*) as m_count, merge_keys FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1),"
11
11 11
group by merge_keys having count(*)=1 ) OnlyInSubset INNER JOIN ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1),"
"
"m_data ON OnlyInSubset.merge_keys = m_data.merge_keys")
tdv_view_name2-paste0("ONLY_IN_SUBSET_VIEW2")
m_body = paste0("[{\"parent Path\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\",\"name\":\"",tdv_view_name2, "\",\"sql\": \""
,tdv_query,
"\\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 2nd - VIEW CREATED ONLY IN SUBSET if(m_result==""){
print("2nd view created successfully.")
#print("Creating a 3rd view now..")
m_path =paste(tdv_rest_base_url,"/dataview/v1") tdv_query =paste("select ",tdv_all_col_names,"
FILE_NAME,m_data.LAYER,MERGE_ALL,m_data.MERGE_KEYS,CONCAT('Duplicateln_',m_data.LAYER) AS
RECONCILE_TYPE,m_count
AS M_COUNT","",
"FROM (select count(*) as m_count, merge_keys, layer FROM
",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," ",
group by merge_keys,layer having count(*)>=2) Duplicateln_Subsets","",
"INNER JOIN "," ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," m_data
11 11 11
ON Duplicateln_Subsets.merge_keys = m_data.merge_keys and Duplicateln_Subsets.layer = m_data.layer") tdv_view_name3=paste0("DUPLICATE_IN_SUBSET_VIEW3")
m_body = paste("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\":\"",tdv_view_name3,
"\",\"sql\": \""
,tdv_query,
"\", \"IfNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 3rd - VIEW CREATED - DUPLICATE IN SUBSET if(m_result==""){
print("3rd view created successfully.")
#print("Creating a 4rth view now..")
m_path =paste(tdv_rest_base_url,"/dataview/v1") tdv_query=paste("SELECT",tdv_all_col_names,"
FILE_NAME,LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,M_COUNT FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name2)," ",
||
11
UNION ALL
11 11 11
}
SELECT",tdv_all_col_names," FILE_NAME,LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,M_COUNT
FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name3))
tdv_view_name4-paste0("ONLY_UNION_DUPLICATE_VIEW4")
m_body = paste0("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\",\"name\":\"",tdv_view_name4,
"\",\"sql\": \""
,tdv_query,
"\\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 4rth - VIEW CREATED ONLYIN UNION DUPLICATE IN SUBSET if(m_result==""){
print("4rth view created successfully.")
#print("Creating a 5th view now..")
m_path =paste(tdv_rest_base_url,"/dataview/v1")
tdv_all_col_names_left_table = paste0(str_replace_all(paste("Left_table.",substr(trimws(tdv_all_col_names, which = "both"),1,nchar(trimws(tdv_all_col_names, which = "both"))-1)),",",", Left_table."),",")
tdv_all_col_names_left_table = ifelse(tdv_all_col_names==" "," ",tdv_all_col_names_left_table)
#print(tdv_all_col_names_left_table)
tdv_query=paste("select
",tdv_all_col_names_left_table,"Left_table.FILE_NAME, Left_table.LAYER, Left_table. MERGE_ALL, Left_table.MERGE_KEYS,Righ
t_Table.RECONCILE_TYPE","",
11
11
FROM ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," Left_table","
" left join ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name4), " Right_Table","
1111
11
ON Left_table.MERGE_KEYS-Right_Table.MERGE_KEYS and Left_table.LAYER=Right_Table.LAYER and Right_Table.RECONCILE_TYPE IS NULL")
tdv_view_name5-paste0("PASS_FAIL_SUBSET_VIEW5")
m_body = paste("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\": \"",tdv_view_name5,
"\",\"sql\": \""
,tdv_query,
"\",\"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 5th - VIEW CREATED - PASS & FAIL SUBSET if(m_result==""){
print("5th view created successfully.")
#print("Creating a 6th view now..")
m_path =paste0(tdv_rest_base_url,"/dataview/v1")
tdv_query= paste0("select
",tdv_all_col_names,"FILE_NAME,LAYER,m_data.MERGE_ALL, MERGE_KEYS, 'PASS' AS
RECONCILE_TYPE, (m_count/m_count) AS M_COUNT","",
"FROM (select merge_all,count(*) as m_count","
11
"
"FROM ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," ",
"group by merge_all having count(*) >=2) pass_subset","
" INNER JOIN ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," m_data", "ON pass_subset.merge_all=m_data.merge_all")
tdv_view_name 6-paste0("PASS_SUBSET_VIEW6")
m_body = paste0("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\":\"",tdv_view_name6,
"\",\"sql\": \""
,tdv_query,
"\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 6th - VIEW CREATED - PASS SUBSET if(m_result ==""){
print("6th view created successfully.")
#print("Creating a 7th view now..")
m_path =paste0(tdv_rest_base_url,"/dataview/v1")
tdv_query=paste("select
",tdv_all_col_names,"FILE_NAME,LAYER, fail_subset. MERGE_ALL, fail_subset. MERGE_KEYS, 'FAIL' AS RECONCILE_TYPE,(m_count/m_count) AS M_COUNT"
"FROM (select MERGE_ALL, MERGE_KEYS, count(*) as m_count "," ",
"FROM ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," "
"GROUP BY MERGE_ALL, MERGE_KEYS having count(*) =1) fail_subset "," ",
" INNER JOIN ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name1)," m_data",
11
ON fail_subset.MERGE_ALL=m_data.MERGE_ALL and fail_subset. MERGE_KEYS-m_data.MERGE_KEYS")
tdv_view_name7-paste0("FAIL_SUBSET_VIEW7")
m_body = paste("[{\"parent Path\":\"", paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\":\"",tdv_view_name7,
"\",\"sql\": \""
,tdv_query,
"\\", \"IfNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 7th - VIEW CREATED - FAIL SUBSET if(m_result=""){
print("7th view created successfully.")
#print("Creating a 8th view now..")
m_path =paste0(tdv_rest_base_url, "/dataview/v1")
tdv_query=paste("SELECT
",tdv_all_col_names,"FILE_NAME, LAYER, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,M_COUNT FROM", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name4),
11
UNION",
"SELECT",tdv_all_col_names,"FILE_NAME,LAYER, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,M_COUNT
FROM ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name6),
" UNION",
"SELECT",tdv_all_col_names,"FILE_NAME,LAYER, MERGE_ALL,MERGE_KEYS, RECONCILE_TYPE,M_COUNT
FROM ", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name7))
tdv_view_name8=paste("CONSOLIDATE_VIEW8")
m_body = paste0("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\": \"",tdv_view_name8,
"\",\"sql\": \""
,tdv_query,
"\", \"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 8th - VIEW CREATED - CONSOLIDATE ALL SUBSET if(m_result ==""){ # For 9th View
print("8th view created successfully.")
#print("Creating a 9th view now..")
m_path =paste0(tdv_rest_base_url, "/dataview/v1")
tdv_query= paste0("SELECT FILE_NAME,LAYER, RECONCILE_TYPE,COUNT(*) as M_COUNT FROM", paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name8),
"GROUP BY FILE_NAME,LAYER, RECONCILE_TYPE")
tdv_view_name9-paste0("TEST_SUMMARY_VIEW9")
m_body = paste0("[{\"parentPath\":\"",paste0(tdv_parent_path,"/",tdv_folder_name),
"\", \"name\":\"",tdv_view_name9,
"\",\"sql\": \""
,tdv_query,
"\",\"ifNotExists\":true,\"annotation\":\"Via REST api\" }]") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 9th - VIEW CREATED - TEST SUMMARY ALL SUBSET if(m_result ==""){
print(paste("Lets now execute the ",tdv_result_type," view"))
m_path =paste(tdv_rest_base_url,"/execute/v1/actions/query/invoke")
if(tdv_result_type=="Summary"){
tdv_query = paste0("SELECT * FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name)) }else{
}
## DETAILED RESULTS
tdv_query = paste0("SELECT * FROM ",paste0(tdv_parent_path,"/",tdv_folder_name,"/",tdv_view_name8))
m_body paste0("{\"standardSQL\": false, ",
=
"\"query\": \"",tdv_query,
"\",\"system\":\"false" "\"}") ####
m_result = getTDV_RESTAPI("POST",m_path,m_body) ## 9th - VIEW CREATED - TEST SUMMARY ALL SUBSET #View(m_result)
if(class(m_result)=="data.frame"){
print(paste0("Successfully Invoked the Test ",tdv_result_type," View"))
}else{
}
#print(m_result)
print(paste("Error Invoking Test ",tdv_result_type," View"))
}else{
print("Error in 9th view response.")
}
}else{
print("Error in 8th view response.")
}
}else{
print("Error in 7th view response.")
}
}else{
print("Error in 6th view response.")
}
}else{
print("Error in 5th view response.")
}
}else{
print("Error in 4rth view response.")
}
}else{
}
print("Error in 3rd view response.")
}else{
}
print("Error in 2nd view response.")
}else{
}
print("Error in 1st view response.")
# }else{
# print("Error in Folder response.")
# }
return(m_result)
} ## End function create_TDV_RESULT_VIEWS
###
#####
parseTDV_parentPath = function (m_path,m_slashes){
#print(m_slashes)
if(m_path != ""){
temp = data.frame(split Names-unlist(strsplit(trimws(m_path, which = "both"),"/"))) temp$mod_split Names=ifelse(stri_detect(temp$split Names, regex = '-| |\\.'),paste0(m_slashes,temp$split Names,m_slashes), temp$splitNames) m_path = paste0(temp$mod_split Names, collapse = "/")
}
#print(class(m_path))
print(m_path)
return(m_path)
}
#################
check_TDV_SCHEMA_TABLE_EXISTENCE =
function(tdv_soap_base_url,tdv_rest_base_url,tdv_publish_path,tdv_datasource_path, schemaName,tableName,this_connect
ion_for_layer_seq){
tdv_cat_schema=""
-
m_connection_String = buildConnectionString(this_DBConnection_Details = this_connection_for_layer_seq) q_schemaName = ifelse(is.na(schemaName), 'IS NULL', paste0("like '%", schemaName,"%'"))
tdv_Query = paste0("select PARENT_PATH FROM /services/databases/system/model/ALL_TABLES", "WHERE table_name="",tableName," and ",
"SCHEMA_NAME ",q_schemaName," and ".
"DATASOURCE_NAME = 'DLTP_Database"")
#m_data = dbGetQuery(conn, tdv_Query)
m_data = connectDB_GetResult(m_connection_String,tdv_Query,this_connection_for_layer_seq)
#tdv_path_to_table = connectDB_GetResult(m_connection_String,tdv_SQLToGetPath,this_connection_for_layer_seq) if(nrow(m_data)==0){ # TABLE NOT PUBLISHED
if(is.na(schemaName)){
}else{
tdv_Query = paste0("select CATALOG_NAME FROM /services/databases/system/model/ALL_SCHEMAS",
"where SCHEMA_NAME ",q_schemaName)
#m_data = dbGetQuery(conn, tdv_Query)
m_data = connectDB_GetResult(m_connection_String,tdv_Query,this_connection_for_layer_seq) tdv_CatalogName = as.character(m_data[1,"CATALOG_NAME"])
tdv_cat_schema-ifelse(is.na(tdv_CatalogName),schemaName,paste0("\\\"",tdv_CatalogName,"\\\"/",schemaName))
# CREATE VIRTUAL SCHEMA IS NOT AVAILBLE
m_path = paste0(tdv_rest_base_url,"/schema/v1/virtual")
m_body = paste("[{\"path\":\"",paste0(tdv_publish_path,"/",tdv_cat_schema),"\",
"\"ifNotExists\":true}]")
11
}
m_result = getTDV_RESTAPI("POST",m_path,m_body)
ifelse(is.na(m_result)==FALSE,"Schema Published","Schema NOT Published")
newViewPath
=
11 11
tdv_Query = paste0("select PARENT_PATH FROM
/services/databases/system/model/ALL_TABLES",
"where table_name="",tableName," and ", "SCHEMA_NAME ",q_schemaName," and ", "DATASOURCE_NAME!= 'DLTP_Database"")
#m_data = dbGetQuery(conn, tdv_Query)
m_data = connectDB_GetResult(m_connection_String,tdv_Query,this_connection_for_layer_seq) if(nrow(m_data)>0){ #TABLE NOT PUBLISHED BUT AVAILABLE IN LOCAL, SO PUBLISH
## Publish under same catalog & schema
newViewPath = m_data[1,"PARENT_PATH"]
newViewPath = paste0(newViewPath,"/",tableName)
}else{ #TABLE NOT PUBLISHED AND ALSO NOT AVAILABLE IN LOCAL, SO ONBOARD & PUBLISH
# Now, onboard new table to this connection, to this catalog, to this schema
m_scheme Table =
ifelse(is.na(schemaName), tableName, paste0(str_replace_all(tdv_cat_schema, "\\\\\"",""),"/",tableName)) m_scheme Table = paste0("/",m_scheme Table)
resourceSubType = ifelse(str_detect(tableName,".csv"), "DELIMITED_FILE_TABLE","DATABASE_TABLE") m_path = paste0(tdv_soap_base_url,"/resource/resource Port.ws")
m_body = paste0("<soap:Envelope
xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"><soap:Body><resource:introspect ResourcesTask
xmlns:resource=\"http://www.compositesw.com/services/system/admin/resource\" xmlns:common=\"http://www.compositesw.com/services/system/util/common\"> <resource:path>",tdv_datasource_path,"</resource:path>
)
<resource:plan>
<resource:entries>
<resource:entry>
<resource:resourceld>
<resource:path>",m_schemeTable,"</resource:path>
<resource:type>TABLE</resource:type>
<resource:subtype>", resourceSubType,"</resource:subtype>
</resource:resourceld>
<resource:action>ADD_OR_UPDATE</resource:action>
</resource:entry>
</resource:entries>
</resource:plan>
<resource:runInBackgroundTransaction>1</resource:runInBackgroundTransaction>
</resource:introspect ResourcesTask></soap:Body></soap:Envelope>")
m_response = POST(url=m_path,
body=m_body,
authenticate("admin", "password", type = "basic"),
add_headers(Accept="text/xml",
Accept="multipart/*",
'Content-Type" = "text/xml"
SOAPAction='introspect ResourcesTask')
rm=content(m_response, as="text", encoding="UTF-8")
x = read_xml(rm)
y=as_list(x)
m_taskID = y$Envelope$Body$introspect ResourcesTaskResponse$taskld[[1]]
if(is.null(m_taskID)==FALSE){
print("Database introspected and added as Table. Now Publishing it.") if(is.na(schemaName)){
newViewPath = paste0(tdv_datasource_path,"/",tableName)
}else{
newViewPath = paste0(tdv_datasource_path,"/",tdv_cat_schema,"/",tableName)
}
}
}else{
}
i=1
print("Database introspected and Table NOT found. Check Table Name in CONFIG file !!!")
for(i in 1:1000000000){next}
# CREATE VIRTUAL TABLE IS NOT AVAILBLE
m_path = paste0(tdv_rest_base_url,"/link/v1") ## PUBLISH AGAIN m_tdv_publish_path=""
if(is.na(schemaName)){
11
m_tdv_publish_path = paste0(tdv_publish_path,"/",tableName)
}else{
}
m_tdv_publish_path = paste0(tdv_publish_path,"/",tdv_cat_schema,"/",tableName)
m_tdv_publish_path = parseTDV_parentPath(m_tdv_publish_path,"\\\"'") newViewPath = parseTDV_parentPath(newViewPath,"\\\"") m_body=paste("[{\"targetPath\": \"", newViewPath,"\",",
"\"isTable\": true,",
"\"path\": \"", m_tdv_publish_path,"\"",",\"IfNotExists\": true}]")
#print(m_body)
m_result = getTDV_RESTAPI("POST",m_path,m_body)
ifelse(is.na(m_result)==FALSE,"Published","NOT Published")
}else{ #TABLE ALREADY PUBLISHED
}
print("Move to next schema & table")
#dbDisconnect(conn)
#####
TDV_replaceTable PathInSQL = function (m_connection_String, referenceSQL,this_connection_for_layer_seq){
#m_referenceSQL = str_to_upper(referenceSQL)
a=data.frame(SQL= strsplit(referenceSQL,split = "FROM")[[1]])
a=a[!str_detect(a$SQL,"SELECT"),"SQL"]
a=ifelse(str_detect(a,"WHERE")==TRUE,a[which (str_detect(a,"WHERE")==TRUE)],a)
#unlist(a)
b=data.frame(SQL= unlist(strsplit(a,split = "WHERE")))
b=b[!str_detect(b$SQL,'\\='),"SQL"]
c=data.frame(SQL= trimws(strsplit(b,split = ","), which = "both"))
d=data.frame(SQL= unlist(str_split(c$SQL," ")))
d=d[str_detect(d$SQL,'\\.'), "SQL"]
if(length(d)>0){
i=1
for(i in 1:length(d)){
m_schema_table=unlist((strsplit(d[i],split = "\\.")))
schemaName=as.character(m_schema_table[1])
tableName=as.character(m_schema_table[2])
tdv_SQLToGetPath = paste0("select PARENT_PATH FROM /services/databases/system/model/ALL_TABLES",
"where table_name="",tableName," and ",
"SCHEMA_NAME like '%", schema Name,"%' and ",
"DATASOURCE_NAME != 'DLTP_Database"")
#tdv_path_to_table =dbGetQuery(conn, tdv_SQLToGetPath)
tdv_path_to_table = connectDB_GetResult(m_connection_String,tdv_SQLTOGetPath, this_connection_for_layer_seq) tdv_path_to_table = as.character(tdv_path_to_table[1])
tdv_path_to_table = paste0(parseTDV_parentPath(tdv_path_to_table, "\\\\\""),"/",tableName)
#print(paste0("BEFORE......", referenceSQL))
referenceSQL = str_replace(string = referenceSQL, pattern = paste0(schemaName,".", tableName), replacement = tdv_path_to_table)
referenceSQL= str_replace_all(referenceSQL,"\n"," ")
#print(paste0("AFTER......", referenceSQL))
}
}
}
return(referenceSQL)
###################
create_HASHCol_UnionSQL = function(this_layer_DB_Provider,
this_layer_name,
this_layer_SEQ,
schemaName,
tableName,
m_Key_Fields,
m_connection_String,
this_connection_for_layer_seq,
new_view_Path,
df_1stLayerColumnDataType,
all_layer_SQL_Add_Column,
df_SQL_Statements_All_Layers,
tdv_all_col_names){
m_view_Query=NA
m_DataType_Query=NA
if(this_layer_DB_Provider=="TDV"){
m_view_Query = paste0("SELECT TOP 1 * FROM ",paste0(schemaName,".", tableName,"_",this_layer_name)) m_DataType_Query = paste0("SELECT column_name, data_type,column_length,column_precision, column_scale FROM /services/databases/system/ALL_COLUMNS",
"WHERE table_name="",tableName,"_",this_layer_name," and schema_name="",schemaName," and DATASOURCE_NAME='DLTP_Database"")
}else if(this_layer_DB_Provider=="PostgreSQL"){
m_view_Query = paste0("SELECT * FROM ",new_view_Path," LIMIT 1")
m_DataType_Query=NA
#m_DataType_Query = paste0("SELECT column_name,data_type,column_length, column_precision, column_scale FROM /services/databases/system/ALL_COLUMNS",
#
11
WHERE table_name="",tableName,"_",this_layer_name," and schema_name="",schemaName," and DATASOURCE_NAME='DLTP_Database"")
}
df_extractedData = connectDB_GetResult(m_connection_String,m_view_Query,this_connection_for_layer_seq)
if(class(df_extracted Data) == "data.frame"){ ### DB Connected Successfully #if(nrow(df_extractedData)==1){ ## SQL Query resulted in expected Records names(df_extracted Data)= str_to_upper(names(df_extracted Data))
m_Key_Fields=str_to_upper(m_Key_Fields)
m_data = df_extractedData[1,]
#print(m_data)
if(this_layer_SEQ==1){
tdv_all_col_names = paste0(names(m_data), collapse = ",")
## Identify the data type of LAYER 1 column names and store it for verifications against all other layer's column
if(this_layer_DB_Provider=="TDV"){
print("First Layer column Data Types")
#print(str(m_data))
# m_DataType_Query = paste0("SELECT column_name, data_type,column_length,column_precision, column_scale FROM /services/databases/system/ALL_COLUMNS",
#
WHERE table_name="",tableName,"_",this_layer_name," and schema_name="",schemaName," and
DATASOURCE_NAME='DLTP_Database"")
df_1stLayerColumnDataType =
connectDB_GetResult(m_connection_String,m_DataType_Query,this_connection_for_layer_seq)
print(df_1stLayerColumnDataType)
}else{
}
df_1stLayerColumnDataType=NA
m_view_Query = paste0("SELECT",tdv_all_col_names," FROM ",new_view_Path)
}else{
print(paste0("OTHER Layer column Data Types:",this_layer_SEQ))
if(this_layer_DB_Provider=="TDV"){
#print(str(m_data))
# m_DataType_Query = paste0("SELECT column_name, data_type,column_length, column_precision, column_scale FROM /services/databases/system/ALL_COLUMNS",
#
"WHERE table_name="",tableName,"_",this_layer_name," and schema_name="",schemaName," and
DATASOURCE_NAME='DLTP_Database"")
df_OtherLayerColumnDataType =
connectDB_GetResult(m_connection_String,m_DataType_Query,this_connection_for_layer_seq)
df_OtherLayerColumnDataType$casted_column = paste0("CAST(",df_1st LayerColumnDataType$column_name," AS ", ifelse(is.na(df_1st LayerColumnDataType$column_length),
df_1stLayerColumnDataType$data_type,
paste0(df_1stLayerColumnDataType$data_type,"(",df_1stLayerColumnDataType$column_length,
ifelse(is.na(df_1stLayerColumnDataType$column_scale)," paste(", ",df_1st LayerColumn DataType$column_scale)),
")"
),"')")
df_OtherLayerColumnDataType$casted_column=ifelse(df_1stLayerColumn DataType$data_type=="TIMESTAMP",
CHAR(10)), 'yyyy-mm-dd')"),
paste0("PARSE_TIMESTAMP(CAST(",df_1stLayerColumnDataType$column_name," AS
df_OtherLayerColumn DataType$casted_column)
df_OtherLayerColumnDataType$casted_column=ifelse(df_1stLayerColumn DataType$data_type=="INTEGER", paste0("CAST(",df_1stLayerColumnDataType$column_name," AS
",df_1stLayerColumnDataType$data_type,")"),
df_OtherLayerColumnDataType$casted_column)
df_OtherLayerColumnDataType$casted_column = paste0 (df_OtherLayerColumnDataType$casted_column," AS ",
df_1stLayerColumnDataType$column_name)
print(df_OtherLayerColumnDataType$casted_column)
tdv_all_col_names_4_Detailed_Result = paste0(df_OtherLayerColumn DataType$casted_column,collapse = ",") print(tdv_all_col_names_4_Detailed_Result)
m_view_Query = paste0("SELECT",tdv_all_col_names_4_Detailed_Result," FROM ",new_view_Path)
}else if(this_layer_DB_Provider=="PostgreSQL"){
tdv_all_col_names = paste0(names(m_data), collapse = ",")
m_view_Query = paste0("SELECT",tdv_all_col_names," FROM ",new_view_Path)
}
}
#print(paste0("From main function ",tdv_all_col_names))
print("MERGE ALL...")
if(this_layer_DB_Provider=="TDV"){
sql_col_MERGE_ALL = paste0("HASHSHA(",concat_DF_cols_for_TDV (this_layer_DB_Provider,m_data),")"," AS
MERGE_ALL")
#sql_col_MERGE_ALL = paste0("TRIM(",concat_DF_cols_for_TDV(m_data),")"," AS MERGE_ALL")
#print(sql_col_MERGE_ALL)
print(paste0("MERGE KEYS...",m_key_Fields))
#print(paste0(select(m_data, unlist(str_split(m_Key_Fields,",")))))
m_data_subset-select(m_data, unlist(str_split(m_Key_Fields,",")))
sql_col_MERGE_KEYS = paste0("HASHSHA(",concat_DF_cols_for_TDV (this_layer_DB_Provider,m_data_subset),")"," AS
MERGE_KEYS")
}else if(this_layer_DB_Provider=="PostgreSQL"){
sql_col_MERGE_ALL = paste0("MD5(",concat_DF_cols_for_TDV(this_layer_DB_Provider, m_data),")"," AS MERGE_ALL") #sql_col_MERGE_ALL = paste0("TRIM(",concat_DF_cols_for_TDV(m_data),")"," AS MERGE_ALL")
#print(sql_col_MERGE_ALL)
print(paste0("MERGE KEYS ...", m_Key_Fields))
#print(paste0(select(m_data, unlist(str_split(m_Key_Fields,","))))) m_data_subset=select(m_data, unlist(str_split(m_Key_Fields,",")))
sql_col_MERGE_KEYS = paste0("MD5(",concat_DF_cols_for_TDV (this_layer_DB_Provider,m_data_subset),")"," AS
MERGE_KEYS")
}
sql_col_LAYER_NAME = paste("'",this_layer_name, ","AS LAYER") sql_col_LAYER_SEQ= paste0(this_layer_SEQ," AS LAYER_SEQ") sql_col_RECONCILE_TYPE = paste0("NA""," AS RECONCILE_TYPE") sql_col_TABLE_NAME = paste0("",this_layer_result_fileName," final_SQL_columns = paste0(sql_col_MERGE_ALL,",",
sql_col_MERGE_KEYS,","
11
sql_col_LAYER_NAME,",",
sql_col_LAYER_SEQ,",",
sql_col_RECONCILE_TYPE,",",
sql_col_TABLE_NAME," ")
-
11111 11
AS FILE_NAME")
this_layer_SQL_Add_Column = str_replace(string = m_view_Query, pattern = "FROM", replacement = paste(", ",final_SQL_columns," FROM "))
#this_layer_SQL_Add_Column = str_replace(string = this_layer_SQL_Add_Column, pattern=
paste0(schemaName, ".", tableName), replacement = tdv_path_to_table)
all_layer_SQL_Add_Column = paste0 (all_layer_SQL_Add_Column,this_layer_SQL_Add_Column," UNION ALL ") #print(all_layer_SQL_Add_Column)
df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"] =
all_layer_SQL_Add_Column
#print(df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"]) }else{ ## Error connecting Database or SQL query generated is not corrrect, check CONFIG file print(paste("ERROR ! Database connection error!!",this_layer_name, df_extractedData,sep="_"))
}
return(list(nm_df_firstLayerColNameDataType = df_1stLayerColumnDataType,
nm_df_all_layer_SQL_Add_Column=all_layer_SQL_Add_Column,
nm_df_SQL_Statements_All_Layers =df_SQL_Statements_All_Layers,
nm_df_tdv_all_col_names=tdv_all_col_names))
}# function ends
####
### CLear garbage collection
### get the arguments from command prompt #!"C:\Program Files\R\R-3.6.2\bin\Rscript.exe"
FUNCTIONS - END
m_start_time = NA
m_jdbc_driver_path=NA
tdv_rest_base_url = NA #"http://a356a0v00008822.workstations.hbap.adroot.hsbc:9400/rest" tdv_parent_path= NA #"/users/composite/admin/Views"
tdv_publish_path= NA
layers_count=NA
args =
commandArgs(trailingOnly=TRUE)
#print(length(args))
if (length(args)<=1) {
stop("14 arguments are required. Config Excel and Result Folder Path.", call.=FALSE)
#exit()
} else if (length(args)==1) {
# default output file
#args[1]= "C:\\01-
Data\\00_DLTP\\02_SS_SNAP_DELTA_DB2_MongoDB\\01_Inputs \\SS_GCDSAX_SNAP_DELTA_DB2vs MongoDB_DataTesting_
Config_v2.1.xlsx"
c_Config_File_Path = args[1]
assign("CURRENT_FOLDER", args[2], envir = .GlobalEnv)
m_R_lib = args[3]
m_R_repo= args[4]
m_detailed_result_flag=args[5]
assign("m_jdbc_driver_path", args[6], envir = .GlobalEnv)
tdv_rest_base_url = args[7]
tdv_parent_path = args[8]
tdv_publish_path = args[9]
tdv_webservice_base_path = args[10]
assign("BUCKET_FOLDER", args[11], envir = .GlobalEnv)
assign("BUCKET_INPUT_FOLDER", args[12], envir = .GlobalEnv)
assign("GOOGLE_SDK_BIN_PATH", shortPathName(args[13]), envir = .GlobalEnv)
td_ldap_user = args[14]
myPackageAndLibraries(m_R_lib,m_R_repo)
td_ldap_pwd = getPass::getPass("Enter the TD LDAP user's password:") if(Sys.getenv("JAVA_HOME")==""){
}
#Sys.setenv(JAVA_HOME='C:\\Swdtools\\JDK1.8.0_66-X64') # for 64-bit version Sys.setenv(JAVA_HOME-Sys.getenv("JAVA_OPTIONS")) # for 64-bit version
print(Sys.getenv("JAVA_HOME"))
#Sys.setenv(JAVA_HOME=args[9])
m_start_time = Sys.time()
}
# print("
#c_Config_File_Path = "C:\\01-
**
IN DEBUG MODE
**")
Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins\\RECONCILE_CSVVSCSV_DEMO_Config_RD.xlsx"
# assign("CURRENT_FOLDER", "C:\\01-Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins", envir = .GlobalEnv) #m_R_lib = "C:/SWDTOOLS/Rtools/win-library/3.6"
# m_R_repo= "https://efx-nexus.systems.uk.hsbc:8084/nexus/content/repositories/cloud-r-project-proxy/"
#m_detailed_result_flag=TRUE
# assign("m_jdbc_driver_path", "C:\\01-
Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins\\jdbc_drivers\\postgresql11\\postgresql-42.2.20.jar", envir=
.GlobalEnv)
# tdv_rest_base_url = "http://a356a0v00008822.workstations.hbap.adroot.hsbc:9400/rest"
#tdv_parent_path = "/users/composite/admin/Views"
#tdv_publish_path = "/services/databases/DLTP_Database"
# tdv_webservice_base_path = "http://A356A0V00008822.workstations.hbap.adroot.hsbc:9400/services/system/admin"
# assign("BUCKET_FOLDER", "C:\\01-Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins\\REFDATA_BUCKET", envir = .GlobalEnv)
# assign("BUCKET_INPUT_FOLDER", "C:\\01-
Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins\\REFDATA_BUCKET\\Input", envir = .GlobalEnv)
# assign("GOOGLE_SDK_BIN_PATH", shortPathName("C:/Program Files (x86)/Google cloud SDK for Windows-204/bin/"), envir = .GlobalEnv)
#
# print(c_Config_File_Path)
# Sys.setenv(JAVA_HOME='C:\\Swdtools\\JDK1.8.0_66-X64') # for 64-bit version
# if(Sys.getenv("JAVA_HOME")==""){
# #Sys.setenv(JAVA_HOME='C:\\Swdtools\\JDK1.8.0_66-X64') # for 64-bit version
# Sys.setenv(JAVA_HOME=Sys.getenv("JAVA_OPTIONS")) # for 64-bit version
# }
# print(Sys.getenv("JAVA_HOME")) #print("****
***** IN DEBUG MODE *
#print(BUCKET_INPUT_FOLDER)
#print(GOOGLE_SDK_BIN_PATH)
****")
#CURRENT_FOLDER = "C:\\01-Data\\00_DLTP\\000_DLTP\\02_R_Solution_In_Jenkins"
## Read all the sheets of CONFIG Files
df_Global_Parameter_1 = read.xlsx(paste0( c_Config_File_Path), sheet =paste("01_Global_Parameter")) df_Source_Connections_2 = read.xlsx(paste0( c_Config_File_Path),sheet =paste("02_Source_Connections")) df_Data_Lineage_5 = read.xlsx(paste0(c_Config_File_Path), sheet =paste0("05_Data_Lineage"))
-
df_Data_Lineage_5 = df_Data_Lineage_5 %>% filter('InScope?`=="Yes")
#print(df_Data_Lineage_5[,"SOURCE.Attribute.Name"])
df_Data_Lineage_Constraints_Map_6= read.xlsx(paste0( c_Config_File_Path), sheet =paste0("06_Constraints_Mapping")) df_Data_Lineage_Constraints_Map_6 = df_Data_Lineage_Constraints_Map_6 %>% filter(ExecuteFlag?`== "Yes") df_Data_Lineage_Reference_Data_Map_7 = read.xlsx(paste0( c_Config_File_Path), sheet =paste0("07_Reference_Mapping")) #RESULT_FOLDER = paste0(str_split(.libPaths(), "dist")[[1]][1],"dist/test_results")
#df_Global_Parameter_1[1,"TESTRESULT_FOLDER"]
# temp_folder=shell("echo %CD%", intern = TRUE)
# RESULT_FOLDER=paste0(temp_folder[NROW(temp_folder)], '\\RESULTS')
#print("...1.1")
RESULT_FOLDER = toString(CURRENT_FOLDER)
#print("...1.2")
## Step 1: Remove old and existing Result files from RESULT_FOLDER
-
list_of_files_path = list.files (paste0(RESULT_FOLDER,"\\RESULTS"), full.names = TRUE, pattern = "*.csv")
for(m_file in list_of_files_path){
}
file.remove(m_file)
#rm(list_of_files_path,m_file)
#print("...1.3")
## Step 2: Get the relevant information from other config files and create SQL query for each LAYER df_Conn_Validation_In_Scope = inner_join(df_Global_Parameter_1,
df_Source_Connections_2,
by=c("POD_NAME"="POD_Name","ENVIRONMENT"="Environment")) %>%
filter(.,ACTIVE_FLAG=="Yes")
#print("...1.4")
df_Conn_Validation_In_Scope [df_Conn_Validation_In_Scope == "NA"]= NA
#So that data flow sequence for intermediate layer is maintained, 1 means base layer df_Conn_Validation_In_Scope = arrange(df_Conn_Validation_In_Scope,Layer_SEQ)
#print("...1.5")
listofDataSources=NA
listofDataSources= tryCatch({executeCode(df_Conn_Validation_In_Scope,
df_Data_Lineage_5,
df_Data_Lineage_Constraints_Map_6,
df_Data_Lineage_Reference_Data_Map_7)}, error=function(cond) {return(NA)})
if(class(listofDataSources)=="character"){
#print("...19")
#write.Alteryx(NA,2)
#write.Alteryx(NA,3)
df_connection Details = NA
df_SQL_Statements_All_Layers = NA
print("ERROR !! No SQL Queries generated. Check Config Data Lineage or Data Constraint Sheet for consistency.")
exit()
}else{
#print("...20")
#write.Alteryx(listofDataSources[[1]],2)
#write.Alteryx(listofDataSources[[2]],3)
df_connection Details = listofDataSources[[1]]
df_SQL_Statements_All_Layers = listofDataSources[[2]]
write.csv(listofDataSources[[2]], paste (RESULT_FOLDER,"\\RESULTS\\SQLS_",str_replace(basename(c_Config_File_Path), ".xlsx" ,""), ".csv",sep=""))
}
#names(df_SQL_Statements_All_Layers)
## STEP 2: If this_connection_for_layer_seq$ODBC_Provider=="TDV" then make sure all the tables in listed in CONFIG file is onboarded and published
if(str_detect(unique(df_connection Details[, "ODBC_Provider"][1]),"TDV")==TRUE) { # IF TDV
m_contID=1
for(m_contID in 1:nrow(df_SQL_Statements_All_Layers)){ # FOR EACH m_contID
this_all_layer_SQLS = df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID,] schemaName=""
layers_count = sum(str_count(names(df_SQL_Statements_All_Layers),"_SQLs"))
m_layer_count = 1
for(m_layer_count in 1:layers_count) { # FOR EACH LAYER in m_contID
this_connection_for_layer_seq= df_connection Details[df_connection Details$ Layer_SEQ==m_layer_count,]
this_layer_name = toString(this_connection_for_layer_seq$Layer)
tdv_datasource_path = toString(this_connection_for_layer_seq$TDV_DSOURCE_PATH)
this_layer_schemaName = this_all_layer_SQLs[,grep(paste0(this_layer_name, ".Schema"), names(this_all_layer_SQLs))] this_layer_tableName = this_all_layer_SQLS[,grep(paste0(this_layer_name, ".Table"), names(this_all_layer_SQLs))] check_TDV_SCHEMA_TABLE_EXISTENCE (tdv_webservice_base_path,
tdv_rest_base_url,
tdv_publish_path,
tdv_datasource_path,
this_layer_schemaName,
this_layer_tableName,
this_connection_for_layer_seq)
} # for each layer in m_contID ends
} ## for each m_contID ends
} ## if TDV ends
## Step 3: Execute SQL queries and create Test_Results file per Table
m_contID=1
layers_count=0
m_Key Fields=""
RESULT_FLAG = FALSE
NATIVE_CONNECTION=NA
#tdv_rest_base_url = "http://a356a0v00008822.workstations.hbap.adroot.hsbc:9400/rest"
#tdv_parent_path="/users/composite/admin/Views"
#RESULT_FOLDER=""
summary_list= as.data.frame(matrix(data=NA, ncol=4))
names(summary_list) = c("FILE_NAME","LAYER","RECONCILE_TYPE","COUNT")
i=1
for(m_contID in 1:nrow(df_SQL_Statements_All_Layers)){
#m_contID=5
this_all_layer_SQLS = df_SQL_Statements_All_Layers [df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID,] layers_count = sum(str_count(names(df_SQL_Statements_All_Layers),"_SQLs"))
#print(this_all_layer_SQLs)
#print(layers_count)
m_layer_count = 0
this_layer_SEQ=0
all_layer_SQL_Add_Column=""
tdv_all_col_names=""
schemaName=""
this_layer_DB_Provider=""
df_1stLayerColumnDataType=NA
layer1_Structure=NA
#print(df_connectionDetails)
#print(layers_count)
for(m_layer_count in 0:layers_count){
#m_layer_count=1
#cat(m_layer_count)
this_connection_for_layer_seq= df_connection Details[df_connection Details$ Layer_SEQ==m_layer_count,]
this_layer_name = toString(this_connection_for_layer_seq$Layer)
this_layer_SEQ= this_connection_for_layer_seq$Layer_SEQ
this_layer_DB_Provider = this_connection_for_layer_seq$ODBC_Provider
#print(paste("***************",NROW(this_connection_for_layer_seq)))
if(NROW(this_connection_for_layer_seq)>0){
if(this_layer_SEQ==0){
#print(this_layer_name)
NATIVE_CONNECTION = as.data.frame(df_connection Details[df_connection Details$ Layer_SEQ==m_layer_count,]) print("Again....Skipping to next layer !!!");
next()
}
}else{
}
print("Again..1..Skipping to next layer !!!");
next()
#this_result_folder = toString(this_connection_for_layer_seq$TESTRESULT_FOLDER)
-
#RESULT_FOLDER = this_result_folder
this_layer_result_fileName = toString(this_all_layer_SQLS$ResultFileName)
RESULT_FILEPATH = paste0(RESULT_FOLDER,"\\RESULTS\\",this_layer_result_fileName)
tdv_folder_name = str_replace(this_layer_result_fileName, pattern = ".csv","")
this_layer_SQL = this_all_layer_SQLs [,grep(paste0(this_layer_name,"_SQLs"), names(this_all_layer_SQLs))] this_layer_schemaName = this_all_layer_SQLs[,grep (paste0(this_layer_name," .Schema"), names(this_all_layer_SQLs))] this_layer_tableName = this_all_layer_SQLs[,grep(paste0(this_layer_name, ".Table"),names(this_all_layer_SQLs))] schemaName=this_layer_schemaName tableName=this_layer_tableName
## Modify SQL query to have new columns like LAYER, LAYER_SEQ, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE <* SQL Query Execution -START ****
##**
***** ##
#m_Key Fields = toString(this_all_layer_SQLs[,grep(paste0(this_layer_name," Keys"), names(this_all_layer_SQLs))]) if(this_layer_SEQ==1){
m_key_Fields = toString(this_all_layer_SQLs[,grep(paste0(this_layer_name,". Keys"), names(this_all_layer_SQLs))])
}
m_key_Fields = str_to_upper(m_Key_Fields)
m_connection_String = buildConnectionString(this_DBConnection_Details = this_connection_for_layer_seq)
if(this_layer_DB_Provider=="TDV"){ ### IF data provider is TDV
## CREATE A VIEW FOLDER FOR SCHEMA NAME
#create_TDV_VIEW_FOLDER(tdv_parent_path, schemaName,tdv_rest_base_url)
## CREATE A VIEW FOLDER FOR TABLE NAME
create_TDV_VIEW_FOLDER(paste0(tdv_parent_path),tdv_folder_name,tdv_rest_base_url)
## GET TDV PATH FOR THE ORIGINAL TABLE in original SQL query for this layer
#print(paste0("####### BEFORE",this_layer_SQL))
this_layer_SQL_withPath =
TDV_replaceTablePathInSQL(m_connection_String,this_layer_SQL,this_connection_for_layer_seq)
#print(paste0("####### AFTER",this_layer_SQL_withPath))
tdv_SQL_CatalogName = paste0("select CATALOG_NAME FROM /services/databases/system/ALL_SCHEMAS ", "where SCHEMA_NAME="",schemaName,'"'")
tdv_CatalogName = connectDB_GetResult(m_connection_String,tdv_SQL_CatalogName,this_connection_for_layer_seq) #tdv_CatalogName = as.character(tdv_CatalogName[1])
tdv_CatalogName = tdv_CatalogName [1,"CATALOG_NAME"] #print(paste0("CATALOG NAME======",tdv_CatalogName))
## CREATA A BASE VIEW FOR TABLE BASED ON this_layer_SQL and PUBLISH IT new_view_Path = create_AS_IS_VIEW(this_layer_DB_Provider,
this_layer_name,
paste0(tdv_parent_path), tdv_publish_path,
tdv_folder_name,
tdv_CatalogName,
schemaName,
tableName,
tdv_query = this_layer_SQL_withPath, tdv_rest_base_url)
df_list_UnionSQL = create_HASHCol_Union SQL(this_layer_DB_Provider,
this_layer_name, this_layer_SEQ, schemaName,
tableName,
m_Key Fields,
m_connection_String,
this_connection_for_layer_seq,
new_view_Path,
df_1stLayerColumnDataType,
all_layer_SQL_Add_Column,
df_SQL_Statements_All_Layers,
tdv_all_col_names)
df_1stLayerColumnDataType = df_list_UnionSQL$nm_df_firstLayerColNameDataType all_layer_SQL_Add_Column = df_list_UnionSQL$nm_df_all_layer_SQL_Add_Column df_SQL_Statements_All_Layers = df_list_UnionSQL$nm_df_SQL_Statements_All_Layers tdv_all_col_names = df_list_UnionSQL$nm_df_tdv_all_col_names
www
}else{ ### IF data provider is NOT TDV
# if Native is true or laer_seq=0 then UNION the queries from layer then
if(class(NATIVE_CONNECTION)=="data.frame"){ ## if NATIVE DB COnnection & ODBC_Provider is not TDVV
# CREATE VIEW on RDBMS
new_view_Path=paste0("ditp.", tableName," ",this_layer_name)
if(this_layer_DB_Provider=="PostgreSQL"){
}
this_layer_SQL=str_replace_all(this_layer_SQL,"\n"," ")
#print(this_layer_SQL)
tdv_query_del_view = paste0("DROP VIEW ",new_view_Path)
tdv_query_create_view = paste0("CREATE OR REPLACE VIEW ",new_view_Path, " AS ",this_layer_SQL) #print(tdv_query)
db_connection_String = buildConnectionString(NATIVE_CONNECTION)
df_extractedData = connectDB_GetResult(db_connection_String,tdv_query_del_view, NATIVE_CONNECTION) df_extractedData = connectDB_GetResult(db_connection_String,tdv_query_create_view, NATIVE_CONNECTION) if(df_extractedData[1]=="No Data"){
print(paste0("NATIVE: View Created Successfully: ",new_view_Path))
}else{
}
print(paste0("NATIVE: View Creation Failed!! : ",new_view_Path))
## Create a UNION SQL for all the layers
df_list_UnionSQL = create_HASHCol_UnionSQL(this_layer_DB_Provider,
this_layer_name,
this_layer_SEQ,
schemaName,
tableName,
m_key_Fields,
db_connection_String,
NATIVE_CONNECTION,
new_view_Path,
df_1stLayerColumnDataType,
all_layer_SQL_Add_Column,
df_SQL_Statements_All_Layers,
tdv_all_col_names)
df_1stLayerColumnDataType = df_list_UnionSQL$nm_df_firstLayerColNameDataType all_layer_SQL_Add_Column = df_list_UnionSQL$nm_df_all_layer_SQL_Add_Column df_SQL_Statements_All_Layers = df_list_UnionSQL$nm_df_SQL_Statements_All_Layers tdv_all_col_names = df_list_UnionSQL$nm_df_tdv_all_col_names
}else{ ## If NOT A NATIVE DB Connection & May have TDDV string in ODBC_provide #print(this_layer_SQL)
df_extractedData = connectDB_GetResult(m_connection_String,this_layer_SQL,this_connection_for_layer_seq) if(class(df_extractedData) == "data.frame"){
print(paste0("COnnection Successful !! for ",this_layer_name,"_",m_contID,"_RECORD
COUNT=", nrow(df_extracted Data)))
names(df_extracted Data)= str_to_upper(names(df_extracted Data))
#print(paste0("df_extractedData......",
#print(str(df_extractedData))
df_extractedData[df_extractedData[,"CLEARINGCODE"]=="381",]))
if(this_layer_SEQ==1){
layer1_Structure = df_extractedData[1,]
m_data = df_extractedData[1,]
m_data$LAYER = NA
}
m_data$LAYER_SEQ= NA
if(nrow(df_extractedData)>0){
=
df_extractedData[] <- mapply(FUN as,df_extracted Data,sapply(layer1_Structure,class), SIMPLIFY = FALSE) # if more than O records are extracted
df_extractedData$LAYER=this_layer_name
df_extractedData$LAYER_SEQ= this_layer_SEQ
#print(str(df_extractedData))
#print(str(m_data))
m_data = rbind(m_data,df_extractedData)
}else{
}
print("Info: No records is fetched.")
}else{
# if error on data extraction
print(paste("ERROR ! Database connection error!!",this_layer_name,df_extractedData,sep="_"))
}## if extracted data is returned or not
df_extractedData=NA
} # if/else NATIVE
}## NOT TDV
##**
***** SQL Query Execution -END
*
} ## for each Layer per ContrainTID
if(this_layer_DB_Provider=="TDV"){
df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"] =
stri_replace_last (df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID,"UNION_A LL_COL"],replacement = ',regex = "UNION ALL")
11 11
#print(df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"])
#tdv_rest_base_url = "http://a356a0v00008822.workstations.hbap.adroot.hsbc:9400/rest"
#tdv_parent_path="/users/composite/admin/Views"
#tdv_pod_name="GDIT_SPM"
#tdv_env ="SIT"
tdv_folder_name = str_replace(this_layer_result_fileName,pattern = ".csv","") # "WO1SPM_HUB_CUST_PORTF" #tdv_folder_name = paste0(schemaName,"/",tdv_folder_name)
tdv_query =
df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"]
summary_table =
create_TDV_RESULT_VIEWS(paste0(tdv_parent_path),tdv_folder_name,tdv_query,m_tdv_all_col_names
NULL,tdv_rest_base_url,tdv_result_type="Summary")
'drop')
#print(class(summary_table))
#print(nrow(summary_table))
if(class(summary_table)=="data.frame"){
}
if(nrow(summary_table)>0){
}
#print(names(summary_table))
names(summary_table)=c("FILE_NAME","LAYER","RECONCILE_TYPE","COUNT")
summary_list = bind_rows(summary_list,summary_table)
RESULT_FLAG = TRUE
# summary_table = final_results%>% group_by(LAYER, RECONCILE_TYPE) %>% summarize(COUNT = n(), .groups =
# summary_table[,"FILE_NAME"] = this_layer_result_fileName
#
#
#print(paste0("9999999...", m_detailed_result_flag))
if(m_detailed_result_flag==TRUE){
final_results=""
final_results =
create_TDV_RESULT_VIEWS(paste0(tdv_parent_path),tdv_folder_name,tdv_query,tdv_all_col_names,tdv_rest_base_url,tdv_
result_type="Detailed")
#print(names(final_results)) #print(ncol(final_results))
#print(tdv_all_col_names)
#print(c("FILE_NAME", "LAYER", "MERGE_ALL", "MERGE_KEYS", "RECONCILE_TYPE", "COUNT"))
if(class(final_results)=="data.frame"){
if(nrow(final_results)>0){
}
temp_a = unlist(str_split(tdv_all_col_names,","))
temp_b = c("FILE_NAME", "LAYER", "MERGE_ALL", "MERGE_KEYS", "RECONCILE_TYPE", "COUNT") names(final_results)=c(temp_a,temp_b)
final_results = final_results%>% arrange (MERGE_KEYS,LAYER)
write.csv(final_results, RESULT_FILEPATH, row.names = FALSE) print(paste0("Generated results for ...", RESULT_FILEPATH))
}
}else{
} #
=
}else{ ## noT TDV
## START: Decide if NATIVE DB based or R dataframe based processing if(class(NATIVE_CONNECTION)=="data.frame"){
df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_ALL_COL"] stri_replace_last(df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID, "UNION_A
LL_COL"],replacement = "", regex = "UNION ALL")
tdv_query =
df_SQL_Statements_All_Layers[df_SQL_Statements_All_Layers$Constraint_RecordID==m_contID,"UNION_ALL_COL"]
#tdv_query = str_replace_all(tdv_query,'\n','')
db_connection_String = buildConnectionString(NATIVE_CONNECTION)
#################
## DROP UNION View
m_view1_name = "dltp.a1_union_view"
m_view2_name = "dltp.a1_only_in_subset"
m_view3_name = "dltp.a1_dup_in_subset"
#m_view4_name = "dltp.A1_PASS_FAILSUBSET"
m_view4_name = "dltp.a1_pass_subset" m_view5_name = "ditp.a1_fail_subset"
m_view6_name = "dltp.a1_consolidate" m_view7_name = "dltp.a1_test_summary" ### DROP EXISTING RESULTANT VIEWS
m_view_query = paste0("DROP VIEW ",m_view7_name,";",
"DROP VIEW",m_view6_name,";",
"DROP VIEW ",m_view5_name,";",
"DROP VIEW ",m_view4_name,";",
"DROP VIEW",m_view3_name, 11.11
1
"DROP VIEW",m_view2_name,";",
"DROP VIEW ",m_view1_name,";"
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(df_extractedData)
## CREATE UNION View "dltp.A1_UNION_VIEW"
m_view_query = paste0("CREATE OR REPLACE VIEW ",m_view1_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query, NATIVE_CONNECTION) if(df_extractedData=="No Data"){print("Success ! Union View.")}else{print("Failure ! Union View.")}
## CREATE ONLY IN SUBSET View:
=
paste0("SELECT FILE_NAME,LAYER_SEQ,LAYER, MERGE_ALL,U.MERGE_KEYS,CONCAT('OnlyIn_',LAYER)
tdv_query RECONCILE_TYPE,COUNT",
"FROM dltp.A1_union_view U,
(SELECT MERGE_KEYS, count(*) COUNT
FROM dltp.A1_union_view
View.")}
GROUP BY MERGE KEYS
HAVING COUNT(*) = 1) OnlyIn_Subsets
WHERE U.MERGE_KEYS-OnlyIn_Subsets. MERGE_KEYS")
m_view_query = paste0("CREATE OR REPLACE VIEW ", m_view2_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query, NATIVE_CONNECTION) #print(df_extractedData)
if(df_extractedData=="No Data"){print("Success ! Onlyln_Subsets View.")}else{print("Failure ! OnlyIn_Subsets
## CREATE DUPLICATE IN EACH LAYER SUBSET View :
tdv_query = paste("SELECT
FILE_NAME,LAYER_SEQ,U.LAYER, MERGE_ALL, U. MERGE_KEYS, CONCAT('Dupln_',U.LAYER) RECONCILE_TYPE,COUNT", "FROM ditp.A1_union_view U,
View.")}
(SELECT LAYER,MERGE_KEYS, count(*) COUNT
FROM dltp.A1_union_view
GROUP BY LAYER, MERGE_KEYS
HAVING count(*) >=2) Duplicateln_Subsets
WHERE U.LAYER= Duplicateln_Subsets.LAYER AND U.MERGE_KEYS=Duplicateln_Subsets.MERGE_KEYS") m_view_query = paste0("CREATE OR REPLACE VIEW", m_view3_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(df_extracted Data)
if(df_extracted Data=="No Data"){print("Success! Duplicateln Subset View.")}else{print("Failure ! Duplicateln Subset
## CREATE PASS SUBSET
tdv_query = paste0("SELECT FILE_NAME,LAYER_SEQ,LAYER, U.MERGE_ALL, MERGE_KEYS,CONCAT('PASS_',LAYER) RECONCILE_TYPE,COUNT/COUNT COUNT
(
FROM ditp.A1_union_view U,
SELECT MERGE_ALL,count(*) COUNT
FROM (
SELECT LAYER, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE
FROM dltp.A1_union_view U
WHERE U.MERGE_KEYS NOT IN (
(SELECT MERGE_KEYS FROM dltp.A1_ONLY_IN_SUBSET)
UNION
(SELECT MERGE_KEYS FROM dltp.A1_DUP_IN_SUBSET)
) PassFailSubset
GROUP BY MERGE_ALL
HAVING count(*) =", layers_count,"
) PASS_SUBSET
WHERE U.MERGE_ALL-PASS_SUBSET.MERGE_ALL"
m_view_query = paste0("CREATE OR REPLACE VIEW", m_view4_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(df_extractedData)
if(df_extracted Data=="No Data"){print("Success ! PASS Subset View.")}else{print("Failure ! PASS Subset View.")} ## CREATE FAIL SUBSET
tdv_query = paste0("SELECT FILE_NAME,LAYER_SEQ,LAYER,U.MERGE_ALL, MERGE_KEYS,CONCAT('FAIL_',LAYER) RECONCILE_TYPE,COUNT/COUNT COUNT
FROM dltp.A1_union_view U,
SELECT MERGE_ALL,count(*) COUNT
FROM (
SELECT LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE
FROM dltp.A1_union_view U
)
WHERE U.MERGE_KEYS NOT IN (
)
(SELECT MERGE_KEYS FROM dltp.A1_ONLY_IN_SUBSET) UNION
(SELECT MERGE_KEYS FROM dltp.A1_DUP_IN_SUBSET)
) PassFailSubset
GROUP BY MERGE_ALL
HAVING count(*) < ", layers_count,"
) FAIL_SUBSET
WHERE U.MERGE_ALL-FAIL_SUBSET.MERGE_ALL"
m_view_query = paste0("CREATE OR REPLACE VIEW ", m_view5_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query, NATIVE_CONNECTION) #print(df_extractedData)
if(df_extracted Data=="No Data"){print("Success! FAIL Subset View.")}else{print("Failure! FAIL Subset View.")} ## CONSOLIDATE query tdv_all_col_names
# (SELECT STRING_AGG('U.' || column_name, ', ')
# FROM information_schema.columns
# WHERE table_schema = 'dltp' AND
# table_name='a1_union_view' AND
# column_name not in('file_name','layer_seq','layer', 'merge_all', 'merge_keys', 'reconcile_type')) #print(tdv_all_col_names)
tdv_query = paste0("SELECT",tdv_all_col_names,",
)
"Consolidated_Subset.*
(
FROM dltp.A1_union_view U,
11
"
SELECT * FROM ditp.A1_ONLY_IN_SUBSET UNION
SELECT * FROM dltp.A1_DUP_IN_SUBSET UNION
SELECT * FROM dltp.A1_PASS_SUBSET
UNION
SELECT * FROM dltp.A1_FAIL_SUBSET
) Consolidated_Subset
WHERE U.LAYER=Consolidated Subset.LAYER AND
U.MERGE_ALL-Consolidated_Subset.MERGE_ALL AND
U.MERGE_KEYS-Consolidated_Subset. MERGE KEYS"
#print(tdv_query)
m_view_query = paste0("CREATE OR REPLACE VIEW ", m_view6_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(df_extractedData)
if(df_extractedData=="No Data"){print("Success! CONSOLIDATE Subset View.")}else{print("Failure ! CONSOLIDATE
Subset View.")}
## TEST SUMMARY quuery
tdv_query = paste0("SELECT FILE_NAME,LAYER_SEQ,LAYER, RECONCILE_TYPE, count(*) COUNT
FROM
UNION
SELECT * FROM dltp.A1_ONLY_IN_SUBSET
SELECT * FROM ditp.A1_DUP_IN_SUBSET
UNION
SELECT * FROM dltp.A1_PASS_SUBSET
UNION
SELECT * FROM dltp.A1_FAIL_SUBSET
) Consolidated_Subset
GROUP BY FILE_NAME,LAYER_SEQ,LAYER,RECONCILE_TYPE")
m_view_query = paste0("CREATE OR REPLACE VIEW ", m_view7_name," AS ",tdv_query)
df_extractedData = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(df_extractedData)
if(df_extractedData=="No Data"){print("Success ! TEST_SUMMARY Subset View.")}else{print("Failure! TEST_SUMMARY Subset View.")}
#################
## GET SUMMARY RESULTS
m_view_query = paste0("SELECT * FROM ",m_view7_name)
summary_table = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) if(class(summary_table)=="data.frame"){
if(nrow(summary_table)>0){
}
}
#print(names(summary_table))
if(ncol(summary_list)==4){
summary_list$LAYER_SEQ=NA
names(summary_list) = c("FILE_NAME","LAYER_SEQ","LAYER","RECONCILE_TYPE","COUNT")
names(summary_table)=c("FILE_NAME","LAYER_SEQ","LAYER","RECONCILE_TYPE","COUNT")
summary_list = bind_rows(summary_list,summary_table)
print(summary_list)
RESULT_FLAG = TRUE
}
## IF DEATAILED_RRESULT FROM CONFIG IS TRUE THEN FIRE consolidate view
if(m_detailed_result_flag==TRUE){
m_view_query = paste0("SELECT * FROM ",m_view6_name)
#final_results=""
final_results = connectDB_GetResult(db_connection_String,m_view_query,NATIVE_CONNECTION) #print(names(final_results))
#print(ncol(final_results))
#print(tdv_all_col_names)
#print(c("FILE_NAME", "LAYER", "MERGE_ALL", "MERGE_KEYS", "RECONCILE_TYPE", "COUNT"))
if(class(final_results)=="data.frame"){
if(nrow(final_results)>0){
}
#temp_a = unlist(str_split(tdv_all_col_names,","))
# temp_b = c("FILE_NAME", "LAYER", "MERGE_ALL", "MERGE_KEYS", "RECONCILE_TYPE", "COUNT") # names(final_results)=c(temp_a,temp_b)
names(final_results) = str_to_upper(names(final_results))
print(names(final_results))
final_results = final_results%>% arrange (MERGE_KEYS,LAYER)
write.csv(final_results, RESULT_FILEPATH,row.names = FALSE)
print(paste0("Generated results for ...", RESULT_FILEPATH))
}
}else{
} #
}else{ ## Generate Hash values for all fields and key fields for reconcile results
#m_data = as.data.frame(m_data)
names(m_data) = str_to_upper(names(m_data))
#View(m_data)
m_data=m_data[-1,]
if(nrow(m_data)>=1){
# HASH VALUE FOR ALL FIELDS
#print(nrow(m_data))
#print(paste0("m_data......", m_data[m_data[,"CLEARINGCODE"]=="381",]))
# concateate all the row value to keep in HASH_ALL and then md5 it at the same field
m_data_subset = select(m_data,-LAYER,-LAYER_SEQ)
#m_data_subset = apply(m_data_subset,2,function(x)gsub('\\s+', '',x)) #print(paste0("m_data_subset......",
#print("....d.1")
m_data_subset[m_data_subset[,"CLEARINGCODE"]=="381",]))
m_data[,"MERGE_ALL"] = col_concat(m_data_subset,sep= "_") #apply(m_data_subset,1,paste, collapse="_") #print("....d.2")
#print(paste0("Before MERGE_ALL......", m_data[m_data[,"CLEARINGCODE"]=="381",]))
#print(as.character(m_data[,"MERGE_ALL"]))
m_data[,"MERGE_ALL"] = md5(as.character(m_data[,"MERGE_ALL"]))
#print("....d.3")
#print(paste0("After MERGE_ALL......", m_data[m_data[,"CLEARINGCODE"]=="381",])) #HASH VALUE FOR only Key FIELDS
#print(m_Key_Fields)
#print(names(m_data))
m_data_subset-select(m_data, unlist(str_split(m_Key_Fields,",")))
#m_data_subset = apply(m_data_subset,2,function(x)gsub('\\s+', "',x))
m_data[,"MERGE_KEYS"] = col_concat(m_data_subset,sep="_") #apply(m_data_subset, 1, paste, collapse="_") m_data[,"MERGE_KEYS"] = md5(m_data[,"MERGE_KEYS"])
#rm(m_data_subset) #**********
## Assigned RECONCILE TYPE to each row based on matching & non-mismatching HASH values m_data = m_data %>% arrange(MERGE_ALL, LAYER_SEQ)
m_data$RECONCILE_TYPE="NA"
#print(m_data)
OnlyIn_Subsets = m_data %>% group_by(MERGE_KEYS) %>% summarise (COUNT=n(), .groups = 'drop') %>% filter(COUNT == 1)
#print("....d.4")
Onlyin_Subsets = inner_join(m_data,Onlyln_Subsets, "MERGE_KEYS", keep=FALSE) %>%
select(LAYER, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,COUNT)
if(nrow(Onlyln_Subsets)>0){
}
OnlyIn_Subsets[,"RECONCILE_TYPE"] = paste0("OnlyIn_",Onlyin_Subsets[,"LAYER"])
#print("....d.5")
#print(paste0("OnlyIn_Subsets=", nrow(Onlyln_Subsets)))
#View(Onlyln_Subsets) #-
## (1)
Duplicateln_Subsets = m_data %>% filter (!(MERGE_KEYS %in% Onlyln_Subsets[,"MERGE_KEYS"])) Duplicateln_Subsets = Duplicateln_Subsets %>% group_by(LAYER, MERGE_KEYS, MERGE_ALL) %>% summarise(COUNT=n(), .groups = 'drop') %>% filter (COUNT >= 2)
#print("....d.6")
Duplicateln_Subsets =
inner_join(m_data, Duplicateln_Subsets,c("LAYER","MERGE_KEYS","MERGE_ALL"), keep=FALSE) %>% select(LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,COUNT)
if(nrow(Duplicateln_Subsets)>0){
}
#Duplicateln_Subsets[,"COUNT"] = Duplicateln_Subsets[,"COUNT"]/ Duplicate In_Subsets[,"COUNT"] Duplicateln_Subsets[,"RECONCILE_TYPE"] = paste0("Duplicateln_",Duplicateln_Subsets[,"LAYER"])
#print(paste0("Duplicateln_Subsets=", sum (Duplicateln_Subsets[,"COUNT"]))) #View(Duplicateln_Subsets) #---
#print("....d.7")
## (2)
PASS_FAIL_Subsets_MINUS_ONLYIN = m_data %>% filter (!(MERGE_KEYS %in% Onlyln_Subsets[,"MERGE_KEYS"])) PASS_FAIL_Subsets_WITH_DUPS PASS_FAIL_Subsets_MINUS_ONLYIN %>% filter ((MERGE_KEYS %in%
Duplicateln_Subsets[,"MERGE_KEYS"]) &
-
MERGE_ALL %in% Duplicateln_Subsets[,"MERGE_ALL"] &
LAYER %in% Duplicateln_Subsets[,"LAYER"]
)
PASS_FAIL_Subsets_WITHOUT_DUPS PASS_FAIL_Subsets_MINUS_ONLYIN %>% filter(!(MERGE_KEYS %in% MERGE_ALL %in% Duplicateln_Subsets[,"MERGE_ALL"] & LAYER %in% Duplicateln_Subsets[,"LAYER"])
)
PASS_FAIL_Subsets = rbind(PASS_FAIL_Subsets_WITH_DUPS, PASS_FAIL_Subsets_WITHOUT_DUPS)
#
# PASS_FAIL_Subsets = left_join(m_data,Onlyln_Subsets,c("MERGE_KEYS","LAYER"))%>%
# select(-RECONCILE_TYPE.x,-MERGE_ALL.y) %>% filter (is.na(RECONCILE_TYPE.y)) %>%
# select(LAYER, MERGE_ALL.X, MERGE_KEYS,RECONCILE_TYPE.y)
#
# PASS_FAIL_Subsets = left_join (PASS_FAIL_Subsets, Duplicateln_Subsets,c("MERGE_KEYS","LAYER"))%>%
# select(-RECONCILE_TYPE.Y) %>% filter (is.na(RECONCILE_TYPE)) %>%
# select(LAYER, MERGE_ALL.X, MERGE_KEYS, RECONCILE_TYPE)
#names(PASS_FAIL_Subsets)=c("LAYER","MERGE_ALL","MERGE_KEYS","RECONCILE_TYPE") #print(paste0("PASS_FAIL_Subsets=", nrow(PASS_FAIL_Subsets)))
### FINDING PASSED
# PASS_Subsets = filter(m_data, !(MERGE_KEYS %in% (union_all (OnlyIn_Subsets, Duplicateln_Subsets) %>% select(MERGE_KEYS)))) %>%
filter(COUNT==layers_count)
#
select(LAYER, MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE) %>%
group_by(MERGE_ALL) %>% summarise(COUNT=n(), .groups = 'drop') %>%
# #print(paste0("pass_subset count=", nrow(PASS_Subsets)))
# PASS_Subsets inner_join(m_data,PASS_Subsets, by="MERGE_ALL",keep=FALSE) %>% select(LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,COUNT)
# #print(paste0("pass_subset count=", nrow(PASS_Subsets), names(PASS_Subsets)))
#
# #
if(nrow(PASS_Subsets)>0){
}
PASS_Subsets$COUNT = (PASS_Subsets[,"COUNT"]/PASS_Subsets[,"COUNT"]) PASS_Subsets $RECONCILE_TYPE = paste0("PASS_",PASS_Subsets[,"LAYER"])
PASS_Subsets = PASS_FAIL_Subsets %>% group_by(MERGE_ALL, MERGE_KEYS) %>% summarise (COUNT=n(), groups = 'drop') %>% filter (COUNT>=2)
#print("....d.8")
PASS_Subsets = inner_join(PASS_Subsets, PASS_FAIL_Subsets,c("MERGE_ALL","MERGE_KEYS")) %>% select(LAYER,MERGE_ALL, MERGE_KEYS,RECONCILE_TYPE,COUNT)
=
PASS_Subsets = PASS_Subsets %>% group_by(MERGE_ALL, MERGE_KEYS, LAYER) %>% summarise (COUNT=n(), #print("....d.9")
.groups 'drop')
## (3)
if(nrow(PASS_Subsets)>0){
}
PASS_Subsets[,"COUNT"] = PASS_Subsets[,"COUNT"]/ PASS_Subsets [,"COUNT"] PASS_Subsets$RECONCILE_TYPE = paste0("PASS_",PASS_Subsets$LAYER)
#print(paste0("PASS_Subsets=", nrow(PASS_Subsets))) #-
#print("....d.10")
### FINDING FAILED
# FAIL_Subsets = filter(m_data, !(MERGE_KEYS %in% (union_all(Onlyin_Subsets, Duplicateln_Subsets) %>% select(MERGE_KEYS)))) %>% #
layers_count)
#
select(LAYER, MERGE_ALL, MERGE_KEYS,RECONCILE_TYPE) %>%
group_by(MERGE_ALL) %>% summarise (COUNT=n(), .groups = 'drop') %>% filter(COUNT <
# FAIL_Subsets = inner_join(m_data, FAIL_Subsets, by="MERGE_ALL", keep=FALSE) %>% select(LAYER,MERGE_ALL, MERGE_KEYS, RECONCILE_TYPE,COUNT)
#
# if(nrow(FAIL_Subsets)>0){
#
#
#}
FAIL_Subsets$COUNT = (FAIL_Subsets$COUNT/FAIL_Subsets$COUNT) FAIL_Subsets$RECONCILE_TYPE = paste0("FAIL_",FAIL_Subsets$LAYER)
FAIL_Subsets = PASS_FAIL_Subsets %>% group_by(MERGE_ALL, MERGE_KEYS) %>% summarise (COUNT=n(), .groups = 'drop') %>% filter (COUNT==1)
#print("....d.11")
FAIL_Subsets = inner_join(FAIL_Subsets, PASS_FAIL_Subsets,c("MERGE_ALL","MERGE_KEYS")) %>%
select(LAYER,MERGE_ALL, MERGE_KEYS,RECONCILE_TYPE,COUNT)
FAIL_Subsets = FAIL_Subsets %>% group_by(MERGE_ALL,MERGE_KEYS,LAYER) %>% summarise (COUNT=n(),
.groups = 'drop')
## (4)
#print("....d.12")
if(nrow(FAIL_Subsets)>0){
}
FAIL_Subsets[,"COUNT"] = FAIL_Subsets[,"COUNT"]/ FAIL_Subsets[,"COUNT"] FAIL_Subsets$RECONCILE_TYPE = paste0("FAIL_",FAIL_Subsets$LAYER)
#print(paste0("FAIL_Subsets=", nrow(FAIL_Subsets))) #-
#print("....d.13")
### CONSOLIDATE ALL RESULTS
final_results_minus_dups = rbind(OnlyIn_Subsets, PASS_Subsets, FAIL_Subsets) #Duplicateln_Subsets #print(paste0("before final_results=", nrow(final_results_minus_dups)))
#rm(Onlyln_Subsets, Duplicateln_Subsets, PASS_Subsets, FAIL_Subsets) #'
#print("....d.14")
**
final_results = inner_join(m_data,final_results_minus_dups,c("LAYER","MERGE_ALL","MERGE_KEYS")) %>% select(-RECONCILE_TYPE.x) %>% rename (RECONCILE_TYPE = RECONCILE_TYPE.y)
#print(paste0("after final_results=", nrow(final_results)))
final_results_for_dup =
inner_join(m_data, Duplicateln_Subsets,c("LAYER","MERGE_ALL","MERGE_KEYS"), keep=FALSE) %>% select(- RECONCILE_TYPE.x) %>% rename(RECONCILE_TYPE = RECONCILE_TYPE.y)
final_results_for_dup = final_results_for_dup %>% distinct_all()
#print(paste0("after with dups final_results=", nrow(final_results_for_dup)))
#print("....d.15")
#print(names(final_results))
#print(names(final_results_for_dup))
final_results = rbind(final_results, final_results_for_dup)
#print(paste0("final final_results=", nrow(final_results)))
m_col_names = names(final_results)
#m_col_names = str_replace_all(m_col_names,"X.","")
m_col_names = str_replace_all(m_col_names,"\\.","")
#print(m_col_names)
names(final_results)= m_col_names
summary_table = final_results%>% group_by(LAYER_SEQ, LAYER, RECONCILE_TYPE) %>% summarize(COUNT = n(),
.groups = 'drop')
summary_table[,"FILE_NAME"] = this_layer_result_fileName
#summary_table$RECONCILE_TYPE = paste0(summary_table$RECONCILE_TYPE,summary_table$LAYER)
#print("....d.16")
# summary_table$MISSING = (max(summary_table$COUNT) - summary_table$COUNT)
#
#MISSING_Subset = summary_table %>% filter(MISSING>0)
#MISSING_Subset$COUNT MISSING_Subset$MISSING
=
#MISSING_Subset$RECONCILE_TYPE = paste0("MISSING_", MISSING_Subset$LAYER) # summary_table = rbind(summary_table, MISSING_Subset)
# summary_table = summary_table[,-which (names(summary_table)=="MISSING")]
summary_table$RECONCILE_TYPE=paste0(summary_table$LAYER_SEQ,".",summary_table$RECONCILE_TYPE)
print(summary_table)
summary_list = bind_rows(summary_list,summary_table)
#print(paste0("9999999...",m_detailed_result_flag))
if(m_detailed_result_flag==TRUE){
final_results = final_results%>% arrange(MERGE_KEYS,LAYER_SEQ) write.csv(final_results, RESULT_FILEPATH,row.names = FALSE) print(paste0("Generated results for ...", RESULT_FILEPATH))
}else{
}
RESULT_FLAG = TRUE
#rm(final_results,m_col_names,summary_table)#**********
} # if m_data
} ## END: Decide if NATIVE DB based or R dataframe based processing }## not TDV
temp_date = difftime(Sys.time(),m_start_time, unit="mins")
#m_start_time = Sys.time()
print(paste("Executed RECON of ", this_layer_result_fileName, " in =",temp_date, " minutes")) print(memory.size())
}## for each contraintsID
# if(this_connection_for_layer_seq$ODBC_Provider=="TDV") { ### if TDV
#
#
# }else{ # NOT TDV: outside of constraint loop
## Step 4: Consolidate Test results and create Summary Report
if(RESULT_FLAG=="TRUE"){
summary_list = summary_list[-1,]
=
m_layer_names distinct (summary_list, LAYER)
m_layer_names = data.frame("LAYER"= gsub("","",m_layer_names[,"LAYER"]))
m_layer_names = m_layer_names %>%
group_by() %>%
summarise(LAYERS = toString(LAYER)) %>%
ungroup()
#print(summary_list)
#str(summary_list)
summary_list$COUNT = as.integer(summary_list$COUNT)
## Transpose with RECONCILE_TYPE in columns and files in rows and count as values
summary_list_pivot = summary_list %>% group_by(RECONCILE_TYPE,FILE_NAME) %>% summarize (n=sum(COUNT), .groups = 'drop') %>% spread (key=RECONCILE_TYPE,value = n)
#print("....d.17")
-
names(summary_list_pivot) = str_replace_all(names(summary_list_pivot),"\\"","") # allocate O where value is NA
summary_list_pivot[is.na(summary_list_pivot)] <- 0
#str(summary_list_pivot)
#summary_list_pivot = as.integer(summary_list_pivot[,-c("FILE_NAME")]) if("PASS" %in% colnames(summary_list_pivot)){
}
summary_list_pivot[,"PASS"] = summary_list_pivot [,"PASS"]/2
if("FAIL" %in% colnames(summary_list_pivot)){
}
summary_list_pivot[,"FAIL"] = summary_list_pivot[,"FAIL"]/2
#
tg = gridExtra::tableGrob(summary_list_pivot)
h = grid::convertHeight(sum(tg$heights), "mm", TRUE)
w = grid::convertWidth(sum(tg$widths), "mm", TRUE)
scale optimal.scale(w,h, 279, 210) + 0.1 #A4 = 279 x 210 in landscape summaryFileName = str_replace(basename(c_Config_File_Path), ".xlsx",".pdf")
ggplot2::ggsave(paste0(RESULT_FOLDER,"\\TEST_SUMMARY_",summaryFileName), tg, width = 279, height = 210, units =
'mm', scale = scale)
#}
rm(summary_list,m_layer_names,summary_list_pivot,tg,h,w,scale) #
}## if RESULT_FLAG
**
write.csv(df_SQL_Statements_All_Layers, paste (RESULT_FOLDER,"\\RESULTS\\SQLs_", str_replace(basename(c_Config_File_Pat h), ".xlsx",""), ".csv",sep=""))
## Step 5: Update the config sheet with INcremental Data if its INcremental load test
temp_date=NA
whichLayer=NA whichDSN-NA
whichQuery=NA
GP_DATALOAD_TYPE=unique(df_Conn_Validation_In_Scope [1,"DATALOAD_TYPE"])
if(GP_DATALOAD_TYPE=="INCREMENTAL"){ ## Apply CDC logic - Day 1- based on prev_exec_date
}
whichLayer = toString(unique(df_Conn_Validation_In_Scope [1,"DERIVE_RESULTFILENAME_LAYER"]))
whichDSN = df_Conn_Validation_In_Scope [df_Conn_Validation_In_Scope$Layer==which Layer,"ODBC_DSN_Name"] whichQuery = unique(df_Conn_Validation_In_Scope [1,"SQL_GET_SOURCE_SERVER_TIME"])
m_date_time = getDBServerDate (which DSN, whichQuery)
temp_date = as.POSIXct(m_date_time) #print(paste0("m_date_time=",m_date_time))
temp_date = format(temp_date,"%Y-%m-%d %H:%M:%S3")
## to this date reduce he current execution time.Say approximately
df_Global_Parameter_1[1,"GP_PREV_TEST_EXEC_TIME"] = paste0("'",temp_date)
#Load the excel template as a workbook
wb <- loadWorkbook(paste0( c_Config_File_Path))
# Remove filters from all sheets if any
all_sheet_names = wb$sheet_names
= =
#remove filter(wb,sheet = 1:NROW(all_sheet_names)) writeData(wb, sheet = paste0("01_Global_Parameter"),
)
x = df_Global_Parameter_1,
startCol = 1,
startRow = 1,
colNames = TRUE
#name = paste0("01_Global_Parameter")
#Save the new workbook
saveWorkbook(wb, paste0(c_Config_File_Path), overwrite = TRUE)
## Total time to Complete
#m_start_time = Sys.time()
temp_date = difftime(Sys.time(),m_start_time, unit="mins")
#temp_date = format(temp_date,"%Y-%m-%d %H:%M:%S3")
print(paste0("Total time to complete the process is = ",temp_date," minutes")) #################
#rm(list = Is())
gc()